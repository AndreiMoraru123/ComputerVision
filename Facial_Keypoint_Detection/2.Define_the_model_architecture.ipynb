{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv5): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=4608, out_features=1360, bias=True)\n",
      "  (fc2): Linear(in_features=1360, out_features=680, bias=True)\n",
      "  (drop): Dropout(p=0.8, inplace=False)\n",
      "  (fc3): Linear(in_features=680, out_features=136, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## TODO: Define the Net in models.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## TODO: Once you've defined the network, you can instantiate it\n",
    "# one example conv layer has been provided for you\n",
    "from models import Net\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FacialKeypointsDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.key_pts_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.key_pts_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = os.path.join(self.root_dir,\n",
    "                                self.key_pts_frame.iloc[idx, 0])\n",
    "\n",
    "        image = mpimg.imread(image_name)\n",
    "\n",
    "        # if image has an alpha color channel, get rid of it\n",
    "        if(image.shape[2] == 4):\n",
    "            image = image[:,:,0:3]\n",
    "\n",
    "        key_pts = self.key_pts_frame.iloc[idx, 1:].values\n",
    "        key_pts = key_pts.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'keypoints': key_pts}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, utils\n",
    "# tranforms\n",
    "\n",
    "class Normalize(object):\n",
    "    \"\"\"Convert a color image to grayscale and normalize the color range to [0,1].\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "\n",
    "        image_copy = np.copy(image)\n",
    "        key_pts_copy = np.copy(key_pts)\n",
    "\n",
    "        # convert image to grayscale\n",
    "        image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # scale color range from [0, 255] to [0, 1]\n",
    "        image_copy=  image_copy/255.0\n",
    "\n",
    "        # scale keypoints to be centered around 0 with a range of [-1, 1]\n",
    "        # mean = 100, sqrt = 50, so, pts should be (pts - 100)/50\n",
    "        key_pts_copy = (key_pts_copy - 100)/50.0\n",
    "\n",
    "\n",
    "        return {'image': image_copy, 'keypoints': key_pts_copy}\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = cv2.resize(image, (new_w, new_h))\n",
    "\n",
    "        # scale the pts, too\n",
    "        key_pts = key_pts * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'keypoints': key_pts}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        key_pts = key_pts - [left, top]\n",
    "\n",
    "        return {'image': image, 'keypoints': key_pts}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "\n",
    "        # if image has no grayscale color channel, add one\n",
    "        if(len(image.shape) == 2):\n",
    "            # add that third color dim\n",
    "            image = image.reshape(image.shape[0], image.shape[1], 1)\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'keypoints': torch.from_numpy(key_pts)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "\n",
    "## TODO: define the data_transform using transforms.Compose([all tx's, . , .])\n",
    "# order matters! i.e. rescaling should come before a smaller crop\n",
    "data_transform = transforms.Compose([Rescale(250),\n",
    "                                     RandomCrop(224),\n",
    "                                     Normalize(),\n",
    "                                     ToTensor()])\n",
    "\n",
    "# testing that you've defined a transform\n",
    "assert(data_transform is not None), 'Define a data_transform'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset:  3462\n"
     ]
    }
   ],
   "source": [
    "transformed_dataset = FacialKeypointsDataset(csv_file='/data/training_frames_keypoints.csv',\n",
    "                                      root_dir='/data/training/',\n",
    "                                      transform=data_transform)\n",
    "\n",
    "# print some stats about the dataset\n",
    "print('Length of dataset: ', len(transformed_dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 224, 224]) torch.Size([68, 2])\n",
      "1 torch.Size([1, 224, 224]) torch.Size([68, 2])\n",
      "2 torch.Size([1, 224, 224]) torch.Size([68, 2])\n",
      "3 torch.Size([1, 224, 224]) torch.Size([68, 2])\n"
     ]
    }
   ],
   "source": [
    "# iterate through the transformed dataset and print some stats about the first few samples\n",
    "for i in range(4):\n",
    "    sample = transformed_dataset[i]\n",
    "    print(i, sample['image'].size(), sample['keypoints'].size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Batching and loading data\n",
    "\n",
    "Next, having defined the transformed dataset, we can use PyTorch's DataLoader class to load the training data in batches of whatever size as well as to shuffle the data for training the model. You can read more about the parameters of the DataLoader, in [this documentation](http://pytorch.org/docs/master/data.html).\n",
    "\n",
    "#### Batch size\n",
    "Decide on a good batch size for training your model. Try both small and large batch sizes and note how the loss decreases as the model trains. Too large a batch size may cause your model to crash and/or run out of memory while training.\n",
    "\n",
    "**Note for Windows users**: Please change the `num_workers` to 0 or you may face some issues with your DataLoader failing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# load training data in batches\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = DataLoader(transformed_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Before training\n",
    "\n",
    "Take a look at how this model performs before it trains. You should see that the keypoints it predicts start off in one spot and don't match the keypoints on a face at all! It's interesting to visualize this behavior so that you can compare it to the model after training and see how the model has improved.\n",
    "\n",
    "#### Load in the test dataset\n",
    "\n",
    "The test dataset is one that this model has *not* seen before, meaning it has not trained with these images. We'll load in this test data and before and after training, see how your model performs on this set!\n",
    "\n",
    "To visualize this test data, we have to go through some un-transformation steps to turn our images into python images from tensors and to turn our keypoints back into a recognizable range."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# load in the test data, using the dataset class\n",
    "# AND apply the data_transform you defined above\n",
    "\n",
    "# create the test dataset\n",
    "test_dataset = FacialKeypointsDataset(csv_file='/data/test_frames_keypoints.csv',\n",
    "                                             root_dir='/data/test/',\n",
    "                                             transform=data_transform)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# load test data in batches\n",
    "batch_size = 10\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apply the model on a test sample\n",
    "\n",
    "To test the model on a test sample of data, you have to follow these steps:\n",
    "1. Extract the image and ground truth keypoints from a sample\n",
    "2. Wrap the image in a Variable, so that the net can process it as input and track how it changes as the image moves through the network.\n",
    "3. Make sure the image is a FloatTensor, which the model expects.\n",
    "4. Forward pass the image through the net to get the predicted, output keypoints.\n",
    "\n",
    "This function test how the network performs on the first batch of test data. It returns the images, the transformed images, the predicted keypoints (produced by the model), and the ground truth keypoints."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# test the model on a batch of test images\n",
    "\n",
    "def net_sample_output():\n",
    "\n",
    "    # iterate through the test dataset\n",
    "    for i, sample in enumerate(test_loader):\n",
    "\n",
    "        # get sample data: images and ground truth keypoints\n",
    "        images = sample['image']\n",
    "        key_pts = sample['keypoints']\n",
    "\n",
    "        # convert images to FloatTensors\n",
    "        if device.type == 'cuda':\n",
    "            images = images.type(torch.cuda.FloatTensor)\n",
    "        else:\n",
    "            images = images.type(torch.FloatTensor)\n",
    "\n",
    "        # forward pass to get net output\n",
    "        output_pts = net(images)\n",
    "\n",
    "        # reshape to batch_size x 68 x 2 pts\n",
    "        output_pts = output_pts.view(output_pts.size()[0], 68, -1)\n",
    "\n",
    "        # break after first image is tested\n",
    "        if i == 0:\n",
    "            return images, output_pts, key_pts\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 224, 224])\n",
      "torch.Size([10, 68, 2])\n",
      "torch.Size([10, 68, 2])\n"
     ]
    }
   ],
   "source": [
    "# call the above function\n",
    "# returns: test images, test predicted keypoints, test ground truth keypoints\n",
    "test_images, test_outputs, gt_pts = net_sample_output()\n",
    "\n",
    "# print out the dimensions of the data to see if they make sense\n",
    "print(test_images.data.size())\n",
    "print(test_outputs.data.size())\n",
    "print(gt_pts.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def show_all_keypoints(image, predicted_key_pts, gt_pts=None):\n",
    "    \"\"\"Show image with predicted keypoints\"\"\"\n",
    "    # image is grayscale\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.scatter(predicted_key_pts[:, 0], predicted_key_pts[:, 1], s=20, marker='.', c='m')\n",
    "    # plot ground truth points as green pts\n",
    "    if gt_pts is not None:\n",
    "        plt.scatter(gt_pts[:, 0], gt_pts[:, 1], s=20, marker='.', c='g')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1440x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x720 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGwAAAB7CAYAAACRtWXuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAIpklEQVR4nO2cz28cRRbHPz0eJooN2MkSBxTvRggSgkgkkGCD8Go1hOvuZbURmytckPgHuK1yQtobB/4AOETsZdFekQlNWBQgBzhAuGVZYUVkvAG3nZlEsce9h66xe3q7x/2jqrueXd/LTM+Mq6rrW+/V9716bS8MQxzkoNX0AByKwREmDI4wYXCECYMjTBgcYcLgCBMGR5gwOMKEod30AJqA7/kt4AjQ64ZdUamefWdhiqzLwDLwiboWA1GD1YQjwCKRd1lU12KwHwnrAZ8Dm+q11+xwisHbj9l6yXuYtYRJnlSTsNIl6hYGvudP+Z7/jO/5no7xNQkrCUOjMPA9fwq4DXwL/KKuxcJWwnQKg1PArHo/q67FwkrC1J51DlgAXq64h10HAvU+UNdiYa3o0AnlBk8B16ULGOsJk64WdY/fSpc4gs1pJN/zW77nH52kPE2M35oJyEBSLT5tgzQvQIT2NJjt2fqRWlwE+sA3wOe+559T3zflKtOIuJXyu/j4taTBrLawmFp8FphhZ4LmadZV5go7NKtdQIDoAFBu8BN2VuqrRGS1iSZtoRt201Z43vbHhEEeodCUGBJBGIxPkPooTmCp1avanAc+iLX1CvBx7PpcN+xuVb4BTbCKsCKrtuoKjwmHRWAK8Iis9VmivVKL9eqGNXtYEQmsyR3FhQPs7EfX1WtIROTfk2PJI+lNwRrCyCmBNcY2ceGwFvvcA/4CDNX7sbE0HRsa66zEKsyb8NUSm01QoCOJnjWWRksMjBBWZhUWkMBxYkexWamVrsTEyAVuk5M2ltECpORJgi43akR0qBuLy+5fE+0JWiSwIudpNImD3fbEhEAZKclHsn6f4+9LK09ThCXjJtAsk1P62LZK3dn5lAVYaHFU/fs4jLjEhEt5FQM+P8uFGjphrnqgqu1A1ngcNskSDPX3DBFZI5zuht3vNLSbx22W/j4vagmc60jjJDIhvxCVAwTAIZMTGGsr1x5Vtd9aYohu2N3qht1bhsnaVqVEguA0k8nSGUvVFkPafrySF8kJ+9UubjDv8Uhe5D1GqdzvXiGs6LlT4XOqSa5MxWnnsr6PYYUodpxVrytFXaRVyd9J0L2pl0g0V46jMuLT+EnBru1aT1jGEUiuCdMlLHTFUTrO9YyKjqrpmNjK/hH4PQViORV//Qs9wkJLHJWMHcu0a8zCdLiRxMqOD/TKpPZU358BLwFstba4/MZlglbA+vo6q6urBEHAYDDg2LFjXLhwgbm5OdbW1jhw4AAP/vHBKRKWaSo0KdquSQvTkdWOr8CrZBx5ZPT9W4CQkP6xPisbK9y5c4der8fq6ir37t2j0+nQ7/e5dOkS/X6fmZkZ2q02pEhvU6FJ0XZNqsTKFUNJ9cW4/+9B5grd7rv/m377yp+v0LvRo91us76+zsGDBzl58iTtdpuVlYjIpaUlzpw5w1PzT0GK9E7rp46EQBJGRYfuG0q2N8ntjn775dtf/jS4O+DGv28wnB5yv32fx2ceZ/ahWW7fvk0QBNy8eZPDhw9z9uxZTjx5guk/TH8aa/NlIqse60cNSUsGvgiMxmHqBrTVQ6S0lxmIjn579e2rtKZafLTwEb1OZORt2ry59iY/hD/AQRg+MuSBzgO0Hm4RrAVMM/0KsWy/7/nzKf2Q1XcRFF3UtQTOBl3Hrm53MBiwFq5FZCmtuhlu8s70OzCtfqSm//3/vM/C1gLvee993Apbi+wUrWb1U8nlJz2E7/m7Wmlu0VFWohvI28XbnWeXU2rP8+h7/cSHsVdv/Hq5tczPMz+PWU7aUY6mItHCwiyXhZVZCbsMqpKbzNi7Uies0+lwaHhoPCiAHZLg/74btoZfA88Rs5w0967B5RcWZnlXexWJbuLfLOQeT7/f5/7m/Z0PQnYIir/3ovcnVk7w/cL3L6KxvDoLZaw07x5WWqIXSIwWQe7xdDod7m7dHf8w7hITo3nt1Gu89e5bWygprwRHT/36CFECV9u9FLXS3LK+iZhDx3jOnz8f3urd4ovTX7BxZGPcFSYRwvNXnufa5WteituFnadoZmiojNv65G9VHD9+PNzY2GAwNSB4PcgmLIT2f9sc/edRln9c9lISvjDukRop47ap8tcINjejuW4NWjzmPTa+f8WxAU/84wm8cJvR5N47eh+QsR9XTXbnwZ63sDi8i16LqN7j4bEvQr5ZurgUTDE1lrVI1IlM3MN0nZnteg/7iTAA76I3qll8l2hyv/rwbx/+aW4wV+m8y/f8R4mOgYw+9bJXSgRyI/xrOAS+8y5628p1bjAHFbIWyro+IHraJSzTRl7sOwvLQhUVrOtEOg/2vIXlJaJi1kL7w+dZaMTC6orpUoRAoYcYSvRl/J5qJ6wuNaX6Srqqa8ALpvtNGYc2MpuIw3SUDuRFPJb6iois2h7EU3HZo2g8rWiCsNoeiEskV39Xpt+yqFLxNQki9jCNhZy15UNTKr6GaHh6R4Ssr1M260JG0WjlhSJF1muXzaatzdCxkgwLg/QJLjvpdSpV3RBDWBJVJl2iix1B8vGKbWULtUAyYaUnXVPFUyOw3iVO2qdsK1uoA1YTJlkcgJkFZbtLrDONpRWmCmhtj8NqO7bIQgUr0V5AC5ZbWJ3iIC1XWdFKjChRq/ewupC1V1aN10zsYba7xLqQ5b4quWTdj1uB5S6xRqS6LxvjNecSFaTEdGIIkzKhpiHCJZqKaSRCyo2LDaB1Q4pKrD2AttUFuz0suy8rc5hSLMxITDMBRtJKOiBlD6sb1h5winGJdcPtYQ5a4FyiMDjChMERJgyOMGFwhAmDI0wYHGHC4AgTBkeYMDjChMERJgyOMGFwhAmDI0wYHGHC4AgTBkeYMDjChEFM1dReQpV6EWdhNaNq2bkjrH5UKjt3hNWPSjWPrsytAVTZwxxhwuBcojA4woTBESYMjjBhcIQJgyNMGBxhwuAIE4b/AVm5HmXgoCL8AAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the output\n",
    "# by default this shows a batch of 10 images\n",
    "def visualize_output(test_images, test_outputs, gt_pts=None, batch_size=10):\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        plt.figure(figsize=(20,10))\n",
    "        ax = plt.subplot(1, batch_size, i+1)\n",
    "\n",
    "        # un-transform the image data\n",
    "        image = test_images[i].data   # get the image from it's Variable wrapper\n",
    "        image = image.numpy()   # convert to numpy array from a Tensor\n",
    "        image = np.transpose(image, (1, 2, 0))   # transpose to go from torch to numpy image\n",
    "\n",
    "        # un-transform the predicted key_pts data\n",
    "        predicted_key_pts = test_outputs[i].data\n",
    "        predicted_key_pts = predicted_key_pts.numpy()\n",
    "        # undo normalization of keypoints\n",
    "        predicted_key_pts = predicted_key_pts*50.0+100\n",
    "\n",
    "        # plot ground truth points for comparison, if they exist\n",
    "        ground_truth_pts = None\n",
    "        if gt_pts is not None:\n",
    "            ground_truth_pts = gt_pts[i]\n",
    "            ground_truth_pts = ground_truth_pts*50.0+100\n",
    "\n",
    "        # call show_all_keypoints\n",
    "        show_all_keypoints(np.squeeze(image), predicted_key_pts, ground_truth_pts)\n",
    "\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# call it\n",
    "visualize_output(test_images, test_outputs, gt_pts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "#### Loss function\n",
    "Training a network to predict keypoints is different than training a network to predict a class; instead of outputting a distribution of classes and using cross entropy loss, you may want to choose a loss function that is suited for regression, which directly compares a predicted value and target value. Read about the various kinds of loss functions (like MSE or L1/SmoothL1 loss) in [this documentation](http://pytorch.org/docs/master/_modules/torch/nn/modules/loss.html).\n",
    "\n",
    "### TODO: Define the loss and optimization\n",
    "\n",
    "Next, you'll define how the model will train by deciding on the loss function and optimizer.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "## TODO: Define the loss and optimization\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.0001)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def train_net(n_epochs):\n",
    "\n",
    "    loss_fun = []\n",
    "\n",
    "    # prepare the net for training\n",
    "    net.train()\n",
    "    net.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # train on batches of data, assumes you already have train_loader\n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            # get the input images and their corresponding labels\n",
    "            images = data['image']\n",
    "            key_pts = data['keypoints']\n",
    "\n",
    "            images,key_pts = images.to(device), key_pts.to(device)\n",
    "\n",
    "            # flatten pts\n",
    "            key_pts = key_pts.view(key_pts.size(0), -1)\n",
    "\n",
    "            # convert variables to floats for regression loss\n",
    "            key_pts = key_pts.type(torch.cuda.FloatTensor)\n",
    "            images = images.type(torch.cuda.FloatTensor)\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            output_pts = net(images)\n",
    "\n",
    "            # calculate the loss between predicted and target keypoints\n",
    "            loss = criterion(output_pts, key_pts)\n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward pass to calculate the weight gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # print loss statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss_fun.append(running_loss)\n",
    "\n",
    "            key_pts = key_pts.type(torch.FloatTensor)\n",
    "            images = images.type(torch.FloatTensor)\n",
    "            output_pts = output_pts.type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "            if batch_i % 10 == 9:    # print every 10 batches\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, running_loss/10))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    return loss_fun\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 10, Avg. Loss: 15.771129846572876\n",
      "Epoch: 1, Batch: 20, Avg. Loss: 1.3602543711662292\n",
      "Epoch: 1, Batch: 30, Avg. Loss: 0.6154066562652588\n",
      "Epoch: 1, Batch: 40, Avg. Loss: 0.5971840202808381\n",
      "Epoch: 1, Batch: 50, Avg. Loss: 0.5823954552412033\n",
      "Epoch: 1, Batch: 60, Avg. Loss: 0.6075722724199295\n",
      "Epoch: 1, Batch: 70, Avg. Loss: 0.6646073579788208\n",
      "Epoch: 1, Batch: 80, Avg. Loss: 0.5555822849273682\n",
      "Epoch: 1, Batch: 90, Avg. Loss: 0.547506445646286\n",
      "Epoch: 1, Batch: 100, Avg. Loss: 0.5654133051633835\n",
      "Epoch: 1, Batch: 110, Avg. Loss: 0.579642516374588\n",
      "Epoch: 1, Batch: 120, Avg. Loss: 0.6327441573143006\n",
      "Epoch: 1, Batch: 130, Avg. Loss: 0.5604717046022415\n",
      "Epoch: 1, Batch: 140, Avg. Loss: 0.6177549749612808\n",
      "Epoch: 1, Batch: 150, Avg. Loss: 0.6004032969474793\n",
      "Epoch: 1, Batch: 160, Avg. Loss: 0.6278302639722824\n",
      "Epoch: 1, Batch: 170, Avg. Loss: 0.5681051284074783\n",
      "Epoch: 1, Batch: 180, Avg. Loss: 0.5528335183858871\n",
      "Epoch: 1, Batch: 190, Avg. Loss: 0.6237008035182953\n",
      "Epoch: 1, Batch: 200, Avg. Loss: 0.5684946149587631\n",
      "Epoch: 1, Batch: 210, Avg. Loss: 0.5906780004501343\n",
      "Epoch: 1, Batch: 220, Avg. Loss: 0.4752162605524063\n",
      "Epoch: 1, Batch: 230, Avg. Loss: 0.5608158648014069\n",
      "Epoch: 1, Batch: 240, Avg. Loss: 0.5279175788164139\n",
      "Epoch: 1, Batch: 250, Avg. Loss: 0.520263946056366\n",
      "Epoch: 1, Batch: 260, Avg. Loss: 0.5454947859048843\n",
      "Epoch: 1, Batch: 270, Avg. Loss: 0.5270957022905349\n",
      "Epoch: 1, Batch: 280, Avg. Loss: 0.5313069969415665\n",
      "Epoch: 1, Batch: 290, Avg. Loss: 0.5255536794662475\n",
      "Epoch: 1, Batch: 300, Avg. Loss: 0.4783056199550629\n",
      "Epoch: 1, Batch: 310, Avg. Loss: 0.4807533919811249\n",
      "Epoch: 1, Batch: 320, Avg. Loss: 0.5227043122053147\n",
      "Epoch: 1, Batch: 330, Avg. Loss: 0.4979675680398941\n",
      "Epoch: 1, Batch: 340, Avg. Loss: 0.4975123107433319\n",
      "Epoch: 2, Batch: 10, Avg. Loss: 0.46358575820922854\n",
      "Epoch: 2, Batch: 20, Avg. Loss: 0.4256386816501617\n",
      "Epoch: 2, Batch: 30, Avg. Loss: 0.4756050705909729\n",
      "Epoch: 2, Batch: 40, Avg. Loss: 0.4967785656452179\n",
      "Epoch: 2, Batch: 50, Avg. Loss: 0.5445132166147232\n",
      "Epoch: 2, Batch: 60, Avg. Loss: 0.43016695976257324\n",
      "Epoch: 2, Batch: 70, Avg. Loss: 0.4368117362260818\n",
      "Epoch: 2, Batch: 80, Avg. Loss: 0.4493204265832901\n",
      "Epoch: 2, Batch: 90, Avg. Loss: 0.3969734162092209\n",
      "Epoch: 2, Batch: 100, Avg. Loss: 0.4102446734905243\n",
      "Epoch: 2, Batch: 110, Avg. Loss: 0.38653775453567507\n",
      "Epoch: 2, Batch: 120, Avg. Loss: 0.39651878774166105\n",
      "Epoch: 2, Batch: 130, Avg. Loss: 0.377079077064991\n",
      "Epoch: 2, Batch: 140, Avg. Loss: 0.4044912487268448\n",
      "Epoch: 2, Batch: 150, Avg. Loss: 0.40758430659770967\n",
      "Epoch: 2, Batch: 160, Avg. Loss: 0.4484486639499664\n",
      "Epoch: 2, Batch: 170, Avg. Loss: 0.40229266583919526\n",
      "Epoch: 2, Batch: 180, Avg. Loss: 0.453112056851387\n",
      "Epoch: 2, Batch: 190, Avg. Loss: 0.38320123851299287\n",
      "Epoch: 2, Batch: 200, Avg. Loss: 0.3814101189374924\n",
      "Epoch: 2, Batch: 210, Avg. Loss: 0.37521004378795625\n",
      "Epoch: 2, Batch: 220, Avg. Loss: 0.5103886157274247\n",
      "Epoch: 2, Batch: 230, Avg. Loss: 0.37597050070762633\n",
      "Epoch: 2, Batch: 240, Avg. Loss: 0.3851876944303513\n",
      "Epoch: 2, Batch: 250, Avg. Loss: 0.34515715688467025\n",
      "Epoch: 2, Batch: 260, Avg. Loss: 0.3802486687898636\n",
      "Epoch: 2, Batch: 270, Avg. Loss: 0.3594214528799057\n",
      "Epoch: 2, Batch: 280, Avg. Loss: 0.3326090484857559\n",
      "Epoch: 2, Batch: 290, Avg. Loss: 0.4373581618070602\n",
      "Epoch: 2, Batch: 300, Avg. Loss: 0.41125564873218534\n",
      "Epoch: 2, Batch: 310, Avg. Loss: 0.35011465549468995\n",
      "Epoch: 2, Batch: 320, Avg. Loss: 0.32657461166381835\n",
      "Epoch: 2, Batch: 330, Avg. Loss: 0.3722440987825394\n",
      "Epoch: 2, Batch: 340, Avg. Loss: 0.3398154452443123\n",
      "Epoch: 3, Batch: 10, Avg. Loss: 0.384180149435997\n",
      "Epoch: 3, Batch: 20, Avg. Loss: 0.32763670235872266\n",
      "Epoch: 3, Batch: 30, Avg. Loss: 0.3005536660552025\n",
      "Epoch: 3, Batch: 40, Avg. Loss: 0.28637188076972964\n",
      "Epoch: 3, Batch: 50, Avg. Loss: 0.3169702276587486\n",
      "Epoch: 3, Batch: 60, Avg. Loss: 0.28842882066965103\n",
      "Epoch: 3, Batch: 70, Avg. Loss: 0.30652897506952287\n",
      "Epoch: 3, Batch: 80, Avg. Loss: 0.33244546353816984\n",
      "Epoch: 3, Batch: 90, Avg. Loss: 0.3533997803926468\n",
      "Epoch: 3, Batch: 100, Avg. Loss: 0.331851689517498\n",
      "Epoch: 3, Batch: 110, Avg. Loss: 0.2837806180119514\n",
      "Epoch: 3, Batch: 120, Avg. Loss: 0.3247536063194275\n",
      "Epoch: 3, Batch: 130, Avg. Loss: 0.3505934104323387\n",
      "Epoch: 3, Batch: 140, Avg. Loss: 0.3196598723530769\n",
      "Epoch: 3, Batch: 150, Avg. Loss: 0.39684389531612396\n",
      "Epoch: 3, Batch: 160, Avg. Loss: 0.3872254014015198\n",
      "Epoch: 3, Batch: 170, Avg. Loss: 0.32446383982896804\n",
      "Epoch: 3, Batch: 180, Avg. Loss: 0.314889957010746\n",
      "Epoch: 3, Batch: 190, Avg. Loss: 0.322898642718792\n",
      "Epoch: 3, Batch: 200, Avg. Loss: 0.2848485618829727\n",
      "Epoch: 3, Batch: 210, Avg. Loss: 0.34785026758909227\n",
      "Epoch: 3, Batch: 220, Avg. Loss: 0.2976398214697838\n",
      "Epoch: 3, Batch: 230, Avg. Loss: 0.28627455681562425\n",
      "Epoch: 3, Batch: 240, Avg. Loss: 0.32732921093702316\n",
      "Epoch: 3, Batch: 250, Avg. Loss: 0.28241267055273056\n",
      "Epoch: 3, Batch: 260, Avg. Loss: 0.3933103263378143\n",
      "Epoch: 3, Batch: 270, Avg. Loss: 0.28901787996292116\n",
      "Epoch: 3, Batch: 280, Avg. Loss: 0.28213444352149963\n",
      "Epoch: 3, Batch: 290, Avg. Loss: 0.29600161910057066\n",
      "Epoch: 3, Batch: 300, Avg. Loss: 0.3147100731730461\n",
      "Epoch: 3, Batch: 310, Avg. Loss: 0.3363271072506905\n",
      "Epoch: 3, Batch: 320, Avg. Loss: 0.2747137114405632\n",
      "Epoch: 3, Batch: 330, Avg. Loss: 0.2859407991170883\n",
      "Epoch: 3, Batch: 340, Avg. Loss: 0.25146139562129977\n",
      "Epoch: 4, Batch: 10, Avg. Loss: 0.3432253435254097\n",
      "Epoch: 4, Batch: 20, Avg. Loss: 0.28574408441781995\n",
      "Epoch: 4, Batch: 30, Avg. Loss: 0.30274866968393327\n",
      "Epoch: 4, Batch: 40, Avg. Loss: 0.2767110735177994\n",
      "Epoch: 4, Batch: 50, Avg. Loss: 0.26888768970966337\n",
      "Epoch: 4, Batch: 60, Avg. Loss: 0.25825767368078234\n",
      "Epoch: 4, Batch: 70, Avg. Loss: 0.2725751891732216\n",
      "Epoch: 4, Batch: 80, Avg. Loss: 0.2925603598356247\n",
      "Epoch: 4, Batch: 90, Avg. Loss: 0.3354786142706871\n",
      "Epoch: 4, Batch: 100, Avg. Loss: 0.23407013863325118\n",
      "Epoch: 4, Batch: 110, Avg. Loss: 0.2920426994562149\n",
      "Epoch: 4, Batch: 120, Avg. Loss: 0.2781167268753052\n",
      "Epoch: 4, Batch: 130, Avg. Loss: 0.2503738164901733\n",
      "Epoch: 4, Batch: 140, Avg. Loss: 0.27496244460344316\n",
      "Epoch: 4, Batch: 150, Avg. Loss: 0.2732183739542961\n",
      "Epoch: 4, Batch: 160, Avg. Loss: 0.27917219400405885\n",
      "Epoch: 4, Batch: 170, Avg. Loss: 0.2777841866016388\n",
      "Epoch: 4, Batch: 180, Avg. Loss: 0.2771749749779701\n",
      "Epoch: 4, Batch: 190, Avg. Loss: 0.23777141720056533\n",
      "Epoch: 4, Batch: 200, Avg. Loss: 0.26985868364572524\n",
      "Epoch: 4, Batch: 210, Avg. Loss: 0.276154987514019\n",
      "Epoch: 4, Batch: 220, Avg. Loss: 0.3322169899940491\n",
      "Epoch: 4, Batch: 230, Avg. Loss: 0.22906425446271897\n",
      "Epoch: 4, Batch: 240, Avg. Loss: 0.27476328760385516\n",
      "Epoch: 4, Batch: 250, Avg. Loss: 0.2836455672979355\n",
      "Epoch: 4, Batch: 260, Avg. Loss: 0.3364549070596695\n",
      "Epoch: 4, Batch: 270, Avg. Loss: 0.25755399465560913\n",
      "Epoch: 4, Batch: 280, Avg. Loss: 0.2448722243309021\n",
      "Epoch: 4, Batch: 290, Avg. Loss: 0.27620911300182344\n",
      "Epoch: 4, Batch: 300, Avg. Loss: 0.3010576099157333\n",
      "Epoch: 4, Batch: 310, Avg. Loss: 0.227494078874588\n",
      "Epoch: 4, Batch: 320, Avg. Loss: 0.25607334822416306\n",
      "Epoch: 4, Batch: 330, Avg. Loss: 0.2955826699733734\n",
      "Epoch: 4, Batch: 340, Avg. Loss: 0.27921590209007263\n",
      "Epoch: 5, Batch: 10, Avg. Loss: 0.27887434512376785\n",
      "Epoch: 5, Batch: 20, Avg. Loss: 0.264982371032238\n",
      "Epoch: 5, Batch: 30, Avg. Loss: 0.3046344146132469\n",
      "Epoch: 5, Batch: 40, Avg. Loss: 0.2685503676533699\n",
      "Epoch: 5, Batch: 50, Avg. Loss: 0.27483667582273485\n",
      "Epoch: 5, Batch: 60, Avg. Loss: 0.2619911029934883\n",
      "Epoch: 5, Batch: 70, Avg. Loss: 0.2414134845137596\n",
      "Epoch: 5, Batch: 80, Avg. Loss: 0.2156181365251541\n",
      "Epoch: 5, Batch: 90, Avg. Loss: 0.19787366539239884\n",
      "Epoch: 5, Batch: 100, Avg. Loss: 0.30929648578166963\n",
      "Epoch: 5, Batch: 110, Avg. Loss: 0.24522112905979157\n",
      "Epoch: 5, Batch: 120, Avg. Loss: 0.22986712902784348\n",
      "Epoch: 5, Batch: 130, Avg. Loss: 0.23595631271600723\n",
      "Epoch: 5, Batch: 140, Avg. Loss: 0.28296957165002823\n",
      "Epoch: 5, Batch: 150, Avg. Loss: 0.30061726570129393\n",
      "Epoch: 5, Batch: 160, Avg. Loss: 0.21167318224906922\n",
      "Epoch: 5, Batch: 170, Avg. Loss: 0.260597425699234\n",
      "Epoch: 5, Batch: 180, Avg. Loss: 0.27069946378469467\n",
      "Epoch: 5, Batch: 190, Avg. Loss: 0.2261955678462982\n",
      "Epoch: 5, Batch: 200, Avg. Loss: 0.2512968823313713\n",
      "Epoch: 5, Batch: 210, Avg. Loss: 0.3690028801560402\n",
      "Epoch: 5, Batch: 220, Avg. Loss: 0.26012816876173017\n",
      "Epoch: 5, Batch: 230, Avg. Loss: 0.24555933326482773\n",
      "Epoch: 5, Batch: 240, Avg. Loss: 0.2297409489750862\n",
      "Epoch: 5, Batch: 250, Avg. Loss: 0.23475708365440368\n",
      "Epoch: 5, Batch: 260, Avg. Loss: 0.28660559803247454\n",
      "Epoch: 5, Batch: 270, Avg. Loss: 0.27533848881721495\n",
      "Epoch: 5, Batch: 280, Avg. Loss: 0.26229132115840914\n",
      "Epoch: 5, Batch: 290, Avg. Loss: 0.30382865071296694\n",
      "Epoch: 5, Batch: 300, Avg. Loss: 0.3015560194849968\n",
      "Epoch: 5, Batch: 310, Avg. Loss: 0.2314371310174465\n",
      "Epoch: 5, Batch: 320, Avg. Loss: 0.33461482971906664\n",
      "Epoch: 5, Batch: 330, Avg. Loss: 0.2480827584862709\n",
      "Epoch: 5, Batch: 340, Avg. Loss: 0.2656965345144272\n",
      "Epoch: 6, Batch: 10, Avg. Loss: 0.2748191446065903\n",
      "Epoch: 6, Batch: 20, Avg. Loss: 0.23399809896945953\n",
      "Epoch: 6, Batch: 30, Avg. Loss: 0.20497204661369323\n",
      "Epoch: 6, Batch: 40, Avg. Loss: 0.28308436647057533\n",
      "Epoch: 6, Batch: 50, Avg. Loss: 0.22340225130319596\n",
      "Epoch: 6, Batch: 60, Avg. Loss: 0.24783398658037187\n",
      "Epoch: 6, Batch: 70, Avg. Loss: 0.21987904831767083\n",
      "Epoch: 6, Batch: 80, Avg. Loss: 0.21016035377979278\n",
      "Epoch: 6, Batch: 90, Avg. Loss: 0.2736328482627869\n",
      "Epoch: 6, Batch: 100, Avg. Loss: 0.2285996027290821\n",
      "Epoch: 6, Batch: 110, Avg. Loss: 0.24949678778648376\n",
      "Epoch: 6, Batch: 120, Avg. Loss: 0.23824573308229446\n",
      "Epoch: 6, Batch: 130, Avg. Loss: 0.3813605695962906\n",
      "Epoch: 6, Batch: 140, Avg. Loss: 0.2646433591842651\n",
      "Epoch: 6, Batch: 150, Avg. Loss: 0.24919796735048294\n",
      "Epoch: 6, Batch: 160, Avg. Loss: 0.2537568897008896\n",
      "Epoch: 6, Batch: 170, Avg. Loss: 0.23847629725933076\n",
      "Epoch: 6, Batch: 180, Avg. Loss: 0.22391663491725922\n",
      "Epoch: 6, Batch: 190, Avg. Loss: 0.1807255819439888\n",
      "Epoch: 6, Batch: 200, Avg. Loss: 0.2536701947450638\n",
      "Epoch: 6, Batch: 210, Avg. Loss: 0.2159552365541458\n",
      "Epoch: 6, Batch: 220, Avg. Loss: 0.21453401148319245\n",
      "Epoch: 6, Batch: 230, Avg. Loss: 0.2353820577263832\n",
      "Epoch: 6, Batch: 240, Avg. Loss: 0.21301931291818618\n",
      "Epoch: 6, Batch: 250, Avg. Loss: 0.23574675172567366\n",
      "Epoch: 6, Batch: 260, Avg. Loss: 0.22074179947376252\n",
      "Epoch: 6, Batch: 270, Avg. Loss: 0.2514284312725067\n",
      "Epoch: 6, Batch: 280, Avg. Loss: 0.21286612004041672\n",
      "Epoch: 6, Batch: 290, Avg. Loss: 0.22334687858819963\n",
      "Epoch: 6, Batch: 300, Avg. Loss: 0.22707814872264862\n",
      "Epoch: 6, Batch: 310, Avg. Loss: 0.23195258527994156\n",
      "Epoch: 6, Batch: 320, Avg. Loss: 0.1832514800131321\n",
      "Epoch: 6, Batch: 330, Avg. Loss: 0.21075731813907622\n",
      "Epoch: 6, Batch: 340, Avg. Loss: 0.2380389839410782\n",
      "Epoch: 7, Batch: 10, Avg. Loss: 0.20607232451438903\n",
      "Epoch: 7, Batch: 20, Avg. Loss: 0.2059294432401657\n",
      "Epoch: 7, Batch: 30, Avg. Loss: 0.2588140070438385\n",
      "Epoch: 7, Batch: 40, Avg. Loss: 0.2635422259569168\n",
      "Epoch: 7, Batch: 50, Avg. Loss: 0.2457026347517967\n",
      "Epoch: 7, Batch: 60, Avg. Loss: 0.2174539178609848\n",
      "Epoch: 7, Batch: 70, Avg. Loss: 0.2159365326166153\n",
      "Epoch: 7, Batch: 80, Avg. Loss: 0.232689568400383\n",
      "Epoch: 7, Batch: 90, Avg. Loss: 0.2057451233267784\n",
      "Epoch: 7, Batch: 100, Avg. Loss: 0.25522960275411605\n",
      "Epoch: 7, Batch: 110, Avg. Loss: 0.20619460493326186\n",
      "Epoch: 7, Batch: 120, Avg. Loss: 0.20757559090852737\n",
      "Epoch: 7, Batch: 130, Avg. Loss: 0.19942433387041092\n",
      "Epoch: 7, Batch: 140, Avg. Loss: 0.20135034322738649\n",
      "Epoch: 7, Batch: 150, Avg. Loss: 0.23896390944719315\n",
      "Epoch: 7, Batch: 160, Avg. Loss: 0.23701874911785126\n",
      "Epoch: 7, Batch: 170, Avg. Loss: 0.24130358994007112\n",
      "Epoch: 7, Batch: 180, Avg. Loss: 0.2310520738363266\n",
      "Epoch: 7, Batch: 190, Avg. Loss: 0.22173444777727128\n",
      "Epoch: 7, Batch: 200, Avg. Loss: 0.2610872909426689\n",
      "Epoch: 7, Batch: 210, Avg. Loss: 0.20407816171646118\n",
      "Epoch: 7, Batch: 220, Avg. Loss: 0.28506691455841066\n",
      "Epoch: 7, Batch: 230, Avg. Loss: 0.2040128454566002\n",
      "Epoch: 7, Batch: 240, Avg. Loss: 0.1939295694231987\n",
      "Epoch: 7, Batch: 250, Avg. Loss: 0.22980370372533798\n",
      "Epoch: 7, Batch: 260, Avg. Loss: 0.17589313983917237\n",
      "Epoch: 7, Batch: 270, Avg. Loss: 0.2510888554155827\n",
      "Epoch: 7, Batch: 280, Avg. Loss: 0.2454652115702629\n",
      "Epoch: 7, Batch: 290, Avg. Loss: 0.22730674743652343\n",
      "Epoch: 7, Batch: 300, Avg. Loss: 0.19928488358855248\n",
      "Epoch: 7, Batch: 310, Avg. Loss: 0.18322909921407698\n",
      "Epoch: 7, Batch: 320, Avg. Loss: 0.21169606298208238\n",
      "Epoch: 7, Batch: 330, Avg. Loss: 0.21154178380966188\n",
      "Epoch: 7, Batch: 340, Avg. Loss: 0.19625708758831023\n",
      "Epoch: 8, Batch: 10, Avg. Loss: 0.22997926771640778\n",
      "Epoch: 8, Batch: 20, Avg. Loss: 0.22122221142053605\n",
      "Epoch: 8, Batch: 30, Avg. Loss: 0.23077767342329025\n",
      "Epoch: 8, Batch: 40, Avg. Loss: 0.18578674867749215\n",
      "Epoch: 8, Batch: 50, Avg. Loss: 0.21628319025039672\n",
      "Epoch: 8, Batch: 60, Avg. Loss: 0.19516688734292983\n",
      "Epoch: 8, Batch: 70, Avg. Loss: 0.20967322885990142\n",
      "Epoch: 8, Batch: 80, Avg. Loss: 0.19609784856438636\n",
      "Epoch: 8, Batch: 90, Avg. Loss: 0.2482892282307148\n",
      "Epoch: 8, Batch: 100, Avg. Loss: 0.2134319767355919\n",
      "Epoch: 8, Batch: 110, Avg. Loss: 0.21249731108546258\n",
      "Epoch: 8, Batch: 120, Avg. Loss: 0.23223406374454497\n",
      "Epoch: 8, Batch: 130, Avg. Loss: 0.1873007223010063\n",
      "Epoch: 8, Batch: 140, Avg. Loss: 0.17585152238607407\n",
      "Epoch: 8, Batch: 150, Avg. Loss: 0.21682022511959076\n",
      "Epoch: 8, Batch: 160, Avg. Loss: 0.1932119235396385\n",
      "Epoch: 8, Batch: 170, Avg. Loss: 0.24568981379270555\n",
      "Epoch: 8, Batch: 180, Avg. Loss: 0.21394276320934297\n",
      "Epoch: 8, Batch: 190, Avg. Loss: 0.25314072519540787\n",
      "Epoch: 8, Batch: 200, Avg. Loss: 0.21040034070611\n",
      "Epoch: 8, Batch: 210, Avg. Loss: 0.22841951847076417\n",
      "Epoch: 8, Batch: 220, Avg. Loss: 0.19287444204092025\n",
      "Epoch: 8, Batch: 230, Avg. Loss: 0.21944889724254607\n",
      "Epoch: 8, Batch: 240, Avg. Loss: 0.23610108941793442\n",
      "Epoch: 8, Batch: 250, Avg. Loss: 0.20339874401688576\n",
      "Epoch: 8, Batch: 260, Avg. Loss: 0.22510955184698106\n",
      "Epoch: 8, Batch: 270, Avg. Loss: 0.211826354265213\n",
      "Epoch: 8, Batch: 280, Avg. Loss: 0.1958741918206215\n",
      "Epoch: 8, Batch: 290, Avg. Loss: 0.19340889453887938\n",
      "Epoch: 8, Batch: 300, Avg. Loss: 0.2813118688762188\n",
      "Epoch: 8, Batch: 310, Avg. Loss: 0.234967140853405\n",
      "Epoch: 8, Batch: 320, Avg. Loss: 0.17799367755651474\n",
      "Epoch: 8, Batch: 330, Avg. Loss: 0.21703499257564546\n",
      "Epoch: 8, Batch: 340, Avg. Loss: 0.22291155010461808\n",
      "Epoch: 9, Batch: 10, Avg. Loss: 0.24007024019956588\n",
      "Epoch: 9, Batch: 20, Avg. Loss: 0.22163802534341812\n",
      "Epoch: 9, Batch: 30, Avg. Loss: 0.1966055765748024\n",
      "Epoch: 9, Batch: 40, Avg. Loss: 0.2181798219680786\n",
      "Epoch: 9, Batch: 50, Avg. Loss: 0.22931456714868545\n",
      "Epoch: 9, Batch: 60, Avg. Loss: 0.20224246233701706\n",
      "Epoch: 9, Batch: 70, Avg. Loss: 0.28964771032333375\n",
      "Epoch: 9, Batch: 80, Avg. Loss: 0.195721136033535\n",
      "Epoch: 9, Batch: 90, Avg. Loss: 0.20035169571638106\n",
      "Epoch: 9, Batch: 100, Avg. Loss: 0.19907502681016923\n",
      "Epoch: 9, Batch: 110, Avg. Loss: 0.21128975450992585\n",
      "Epoch: 9, Batch: 120, Avg. Loss: 0.17621147781610488\n",
      "Epoch: 9, Batch: 130, Avg. Loss: 0.1997683271765709\n",
      "Epoch: 9, Batch: 140, Avg. Loss: 0.19135929197072982\n",
      "Epoch: 9, Batch: 150, Avg. Loss: 0.2111910969018936\n",
      "Epoch: 9, Batch: 160, Avg. Loss: 0.20492298379540444\n",
      "Epoch: 9, Batch: 170, Avg. Loss: 0.21494050025939943\n",
      "Epoch: 9, Batch: 180, Avg. Loss: 0.175898939371109\n",
      "Epoch: 9, Batch: 190, Avg. Loss: 0.20797364860773088\n",
      "Epoch: 9, Batch: 200, Avg. Loss: 0.19529906511306763\n",
      "Epoch: 9, Batch: 210, Avg. Loss: 0.17296237498521805\n",
      "Epoch: 9, Batch: 220, Avg. Loss: 0.26482415199279785\n",
      "Epoch: 9, Batch: 230, Avg. Loss: 0.22048212438821793\n",
      "Epoch: 9, Batch: 240, Avg. Loss: 0.3933857575058937\n",
      "Epoch: 9, Batch: 250, Avg. Loss: 0.1958777979016304\n",
      "Epoch: 9, Batch: 260, Avg. Loss: 0.19130023717880248\n",
      "Epoch: 9, Batch: 270, Avg. Loss: 0.20777510702610016\n",
      "Epoch: 9, Batch: 280, Avg. Loss: 0.2364932119846344\n",
      "Epoch: 9, Batch: 290, Avg. Loss: 0.25782019942998885\n",
      "Epoch: 9, Batch: 300, Avg. Loss: 0.18652126640081407\n",
      "Epoch: 9, Batch: 310, Avg. Loss: 0.1864152431488037\n",
      "Epoch: 9, Batch: 320, Avg. Loss: 0.21479050442576408\n",
      "Epoch: 9, Batch: 330, Avg. Loss: 0.14859285578131676\n",
      "Epoch: 9, Batch: 340, Avg. Loss: 0.19544915705919266\n",
      "Epoch: 10, Batch: 10, Avg. Loss: 0.20420248433947563\n",
      "Epoch: 10, Batch: 20, Avg. Loss: 0.21897505968809128\n",
      "Epoch: 10, Batch: 30, Avg. Loss: 0.22841754853725432\n",
      "Epoch: 10, Batch: 40, Avg. Loss: 0.16087129563093186\n",
      "Epoch: 10, Batch: 50, Avg. Loss: 0.15751850008964538\n",
      "Epoch: 10, Batch: 60, Avg. Loss: 0.15971135050058366\n",
      "Epoch: 10, Batch: 70, Avg. Loss: 0.36719730123877525\n",
      "Epoch: 10, Batch: 80, Avg. Loss: 0.21001323238015174\n",
      "Epoch: 10, Batch: 90, Avg. Loss: 0.185109680891037\n",
      "Epoch: 10, Batch: 100, Avg. Loss: 0.24780136942863465\n",
      "Epoch: 10, Batch: 110, Avg. Loss: 0.22859540060162545\n",
      "Epoch: 10, Batch: 120, Avg. Loss: 0.2524164289236069\n",
      "Epoch: 10, Batch: 130, Avg. Loss: 0.18468448221683503\n",
      "Epoch: 10, Batch: 140, Avg. Loss: 0.17319996058940887\n",
      "Epoch: 10, Batch: 150, Avg. Loss: 0.20151093527674674\n",
      "Epoch: 10, Batch: 160, Avg. Loss: 0.20051799416542054\n",
      "Epoch: 10, Batch: 170, Avg. Loss: 0.2143673948943615\n",
      "Epoch: 10, Batch: 180, Avg. Loss: 0.19337251633405686\n",
      "Epoch: 10, Batch: 190, Avg. Loss: 0.224969282746315\n",
      "Epoch: 10, Batch: 200, Avg. Loss: 0.20614443868398666\n",
      "Epoch: 10, Batch: 210, Avg. Loss: 0.21179284676909446\n",
      "Epoch: 10, Batch: 220, Avg. Loss: 0.26366192400455474\n",
      "Epoch: 10, Batch: 230, Avg. Loss: 0.21418203338980674\n",
      "Epoch: 10, Batch: 240, Avg. Loss: 0.20708547234535218\n",
      "Epoch: 10, Batch: 250, Avg. Loss: 0.20179179236292838\n",
      "Epoch: 10, Batch: 260, Avg. Loss: 0.20621316730976105\n",
      "Epoch: 10, Batch: 270, Avg. Loss: 0.19778241217136383\n",
      "Epoch: 10, Batch: 280, Avg. Loss: 0.22207011282444\n",
      "Epoch: 10, Batch: 290, Avg. Loss: 0.20337612330913543\n",
      "Epoch: 10, Batch: 300, Avg. Loss: 0.22772434502840042\n",
      "Epoch: 10, Batch: 310, Avg. Loss: 0.23552109599113463\n",
      "Epoch: 10, Batch: 320, Avg. Loss: 0.19679755866527557\n",
      "Epoch: 10, Batch: 330, Avg. Loss: 0.20736207887530328\n",
      "Epoch: 10, Batch: 340, Avg. Loss: 0.18220619708299637\n",
      "Epoch: 11, Batch: 10, Avg. Loss: 0.19464022517204285\n",
      "Epoch: 11, Batch: 20, Avg. Loss: 0.2038371294736862\n",
      "Epoch: 11, Batch: 30, Avg. Loss: 0.25390017330646514\n",
      "Epoch: 11, Batch: 40, Avg. Loss: 0.17494943290948867\n",
      "Epoch: 11, Batch: 50, Avg. Loss: 0.214143119007349\n",
      "Epoch: 11, Batch: 60, Avg. Loss: 0.1987538866698742\n",
      "Epoch: 11, Batch: 70, Avg. Loss: 0.21441190242767333\n",
      "Epoch: 11, Batch: 80, Avg. Loss: 0.2150951772928238\n",
      "Epoch: 11, Batch: 90, Avg. Loss: 0.2460382752120495\n",
      "Epoch: 11, Batch: 100, Avg. Loss: 0.18503838777542114\n",
      "Epoch: 11, Batch: 110, Avg. Loss: 0.22394405752420427\n",
      "Epoch: 11, Batch: 120, Avg. Loss: 0.1815038375556469\n",
      "Epoch: 11, Batch: 130, Avg. Loss: 0.26354451924562455\n",
      "Epoch: 11, Batch: 140, Avg. Loss: 0.23686863780021666\n",
      "Epoch: 11, Batch: 150, Avg. Loss: 0.24657723903656006\n",
      "Epoch: 11, Batch: 160, Avg. Loss: 0.1847812667489052\n",
      "Epoch: 11, Batch: 170, Avg. Loss: 0.17258669286966324\n",
      "Epoch: 11, Batch: 180, Avg. Loss: 0.1855766147375107\n",
      "Epoch: 11, Batch: 190, Avg. Loss: 0.18097529262304307\n",
      "Epoch: 11, Batch: 200, Avg. Loss: 0.178781346231699\n",
      "Epoch: 11, Batch: 210, Avg. Loss: 0.20734084621071816\n",
      "Epoch: 11, Batch: 220, Avg. Loss: 0.18548786416649818\n",
      "Epoch: 11, Batch: 230, Avg. Loss: 0.218142968416214\n",
      "Epoch: 11, Batch: 240, Avg. Loss: 0.1898573726415634\n",
      "Epoch: 11, Batch: 250, Avg. Loss: 0.17210773080587388\n",
      "Epoch: 11, Batch: 260, Avg. Loss: 0.1620101101696491\n",
      "Epoch: 11, Batch: 270, Avg. Loss: 0.22840612828731538\n",
      "Epoch: 11, Batch: 280, Avg. Loss: 0.22964108884334564\n",
      "Epoch: 11, Batch: 290, Avg. Loss: 0.19064194113016128\n",
      "Epoch: 11, Batch: 300, Avg. Loss: 0.3726689524948597\n",
      "Epoch: 11, Batch: 310, Avg. Loss: 0.21464895457029343\n",
      "Epoch: 11, Batch: 320, Avg. Loss: 0.20266179889440536\n",
      "Epoch: 11, Batch: 330, Avg. Loss: 0.17840529680252076\n",
      "Epoch: 11, Batch: 340, Avg. Loss: 0.19791775047779084\n",
      "Epoch: 12, Batch: 10, Avg. Loss: 0.21088027209043503\n",
      "Epoch: 12, Batch: 20, Avg. Loss: 0.17257587611675262\n",
      "Epoch: 12, Batch: 30, Avg. Loss: 0.2245841145515442\n",
      "Epoch: 12, Batch: 40, Avg. Loss: 0.1718558356165886\n",
      "Epoch: 12, Batch: 50, Avg. Loss: 0.20170472413301468\n",
      "Epoch: 12, Batch: 60, Avg. Loss: 0.19009282812476158\n",
      "Epoch: 12, Batch: 70, Avg. Loss: 0.19485098868608475\n",
      "Epoch: 12, Batch: 80, Avg. Loss: 0.18039027452468873\n",
      "Epoch: 12, Batch: 90, Avg. Loss: 0.2228153720498085\n",
      "Epoch: 12, Batch: 100, Avg. Loss: 0.1579880565404892\n",
      "Epoch: 12, Batch: 110, Avg. Loss: 0.1786009654402733\n",
      "Epoch: 12, Batch: 120, Avg. Loss: 0.19141947254538536\n",
      "Epoch: 12, Batch: 130, Avg. Loss: 0.2192283645272255\n",
      "Epoch: 12, Batch: 140, Avg. Loss: 0.19665734618902206\n",
      "Epoch: 12, Batch: 150, Avg. Loss: 0.36729611083865166\n",
      "Epoch: 12, Batch: 160, Avg. Loss: 0.19067925363779067\n",
      "Epoch: 12, Batch: 170, Avg. Loss: 0.16161180585622786\n",
      "Epoch: 12, Batch: 180, Avg. Loss: 0.26132033616304395\n",
      "Epoch: 12, Batch: 190, Avg. Loss: 0.21742246747016908\n",
      "Epoch: 12, Batch: 200, Avg. Loss: 0.2040259249508381\n",
      "Epoch: 12, Batch: 210, Avg. Loss: 0.20011138096451758\n",
      "Epoch: 12, Batch: 220, Avg. Loss: 0.20392241030931474\n",
      "Epoch: 12, Batch: 230, Avg. Loss: 0.1769842140376568\n",
      "Epoch: 12, Batch: 240, Avg. Loss: 0.1883029595017433\n",
      "Epoch: 12, Batch: 250, Avg. Loss: 0.17872926816344262\n",
      "Epoch: 12, Batch: 260, Avg. Loss: 0.2340312644839287\n",
      "Epoch: 12, Batch: 270, Avg. Loss: 0.16624734848737716\n",
      "Epoch: 12, Batch: 280, Avg. Loss: 0.21236737817525864\n",
      "Epoch: 12, Batch: 290, Avg. Loss: 0.19278420880436897\n",
      "Epoch: 12, Batch: 300, Avg. Loss: 0.19428791254758834\n",
      "Epoch: 12, Batch: 310, Avg. Loss: 0.19961018189787866\n",
      "Epoch: 12, Batch: 320, Avg. Loss: 0.19519567489624023\n",
      "Epoch: 12, Batch: 330, Avg. Loss: 0.22273019179701806\n",
      "Epoch: 12, Batch: 340, Avg. Loss: 0.17671816423535347\n",
      "Epoch: 13, Batch: 10, Avg. Loss: 0.18923094347119332\n",
      "Epoch: 13, Batch: 20, Avg. Loss: 0.20320496037602426\n",
      "Epoch: 13, Batch: 30, Avg. Loss: 0.17459761723876\n",
      "Epoch: 13, Batch: 40, Avg. Loss: 0.22245713770389558\n",
      "Epoch: 13, Batch: 50, Avg. Loss: 0.1872328042984009\n",
      "Epoch: 13, Batch: 60, Avg. Loss: 0.20141816139221191\n",
      "Epoch: 13, Batch: 70, Avg. Loss: 0.2506849244236946\n",
      "Epoch: 13, Batch: 80, Avg. Loss: 0.1839638501405716\n",
      "Epoch: 13, Batch: 90, Avg. Loss: 0.2142127625644207\n",
      "Epoch: 13, Batch: 100, Avg. Loss: 0.17956010550260543\n",
      "Epoch: 13, Batch: 110, Avg. Loss: 0.3344984218478203\n",
      "Epoch: 13, Batch: 120, Avg. Loss: 0.2222258910536766\n",
      "Epoch: 13, Batch: 130, Avg. Loss: 0.1739479996263981\n",
      "Epoch: 13, Batch: 140, Avg. Loss: 0.20584223568439483\n",
      "Epoch: 13, Batch: 150, Avg. Loss: 0.18330744802951812\n",
      "Epoch: 13, Batch: 160, Avg. Loss: 0.22241903990507125\n",
      "Epoch: 13, Batch: 170, Avg. Loss: 0.20288725420832635\n",
      "Epoch: 13, Batch: 180, Avg. Loss: 0.1816727377474308\n",
      "Epoch: 13, Batch: 190, Avg. Loss: 0.19661863669753074\n",
      "Epoch: 13, Batch: 200, Avg. Loss: 0.1908525362610817\n",
      "Epoch: 13, Batch: 210, Avg. Loss: 0.19803286492824554\n",
      "Epoch: 13, Batch: 220, Avg. Loss: 0.21248506158590316\n",
      "Epoch: 13, Batch: 230, Avg. Loss: 0.1985887333750725\n",
      "Epoch: 13, Batch: 240, Avg. Loss: 0.19938987717032433\n",
      "Epoch: 13, Batch: 250, Avg. Loss: 0.24528319835662843\n",
      "Epoch: 13, Batch: 260, Avg. Loss: 0.22345878183841705\n",
      "Epoch: 13, Batch: 270, Avg. Loss: 0.20761363506317138\n",
      "Epoch: 13, Batch: 280, Avg. Loss: 0.1590014085173607\n",
      "Epoch: 13, Batch: 290, Avg. Loss: 0.1850914865732193\n",
      "Epoch: 13, Batch: 300, Avg. Loss: 0.16905810683965683\n",
      "Epoch: 13, Batch: 310, Avg. Loss: 0.20024264305830003\n",
      "Epoch: 13, Batch: 320, Avg. Loss: 0.29217264205217364\n",
      "Epoch: 13, Batch: 330, Avg. Loss: 0.22167311757802963\n",
      "Epoch: 13, Batch: 340, Avg. Loss: 0.20628869086503981\n",
      "Epoch: 14, Batch: 10, Avg. Loss: 0.2498036280274391\n",
      "Epoch: 14, Batch: 20, Avg. Loss: 0.19273430109024048\n",
      "Epoch: 14, Batch: 30, Avg. Loss: 0.17636931762099267\n",
      "Epoch: 14, Batch: 40, Avg. Loss: 0.1765128955245018\n",
      "Epoch: 14, Batch: 50, Avg. Loss: 0.17866719886660576\n",
      "Epoch: 14, Batch: 60, Avg. Loss: 0.19240200221538545\n",
      "Epoch: 14, Batch: 70, Avg. Loss: 0.16393074467778207\n",
      "Epoch: 14, Batch: 80, Avg. Loss: 0.22266172170639037\n",
      "Epoch: 14, Batch: 90, Avg. Loss: 0.2663943782448769\n",
      "Epoch: 14, Batch: 100, Avg. Loss: 0.18982921838760375\n",
      "Epoch: 14, Batch: 110, Avg. Loss: 0.18249511122703552\n",
      "Epoch: 14, Batch: 120, Avg. Loss: 0.13907142654061316\n",
      "Epoch: 14, Batch: 130, Avg. Loss: 0.20749111771583556\n",
      "Epoch: 14, Batch: 140, Avg. Loss: 0.21839887648820877\n",
      "Epoch: 14, Batch: 150, Avg. Loss: 0.22262473106384278\n",
      "Epoch: 14, Batch: 160, Avg. Loss: 0.20767930150032043\n",
      "Epoch: 14, Batch: 170, Avg. Loss: 0.23863361328840255\n",
      "Epoch: 14, Batch: 180, Avg. Loss: 0.20092607140541077\n",
      "Epoch: 14, Batch: 190, Avg. Loss: 0.19506874680519104\n",
      "Epoch: 14, Batch: 200, Avg. Loss: 0.23362695574760436\n",
      "Epoch: 14, Batch: 210, Avg. Loss: 0.22754207998514175\n",
      "Epoch: 14, Batch: 220, Avg. Loss: 0.21569514647126198\n",
      "Epoch: 14, Batch: 230, Avg. Loss: 0.1711529664695263\n",
      "Epoch: 14, Batch: 240, Avg. Loss: 0.19948364794254303\n",
      "Epoch: 14, Batch: 250, Avg. Loss: 0.197643256932497\n",
      "Epoch: 14, Batch: 260, Avg. Loss: 0.1754372663795948\n",
      "Epoch: 14, Batch: 270, Avg. Loss: 0.20994581729173661\n",
      "Epoch: 14, Batch: 280, Avg. Loss: 0.2150919184088707\n",
      "Epoch: 14, Batch: 290, Avg. Loss: 0.19662100970745086\n",
      "Epoch: 14, Batch: 300, Avg. Loss: 0.17767878249287605\n",
      "Epoch: 14, Batch: 310, Avg. Loss: 0.17566949650645255\n",
      "Epoch: 14, Batch: 320, Avg. Loss: 0.21894182935357093\n",
      "Epoch: 14, Batch: 330, Avg. Loss: 0.1906972572207451\n",
      "Epoch: 14, Batch: 340, Avg. Loss: 0.20204904675483704\n",
      "Epoch: 15, Batch: 10, Avg. Loss: 0.17514111697673798\n",
      "Epoch: 15, Batch: 20, Avg. Loss: 0.21261684596538544\n",
      "Epoch: 15, Batch: 30, Avg. Loss: 0.1811772622168064\n",
      "Epoch: 15, Batch: 40, Avg. Loss: 0.16766045689582826\n",
      "Epoch: 15, Batch: 50, Avg. Loss: 0.21477270051836966\n",
      "Epoch: 15, Batch: 60, Avg. Loss: 0.1921516887843609\n",
      "Epoch: 15, Batch: 70, Avg. Loss: 0.2512004926800728\n",
      "Epoch: 15, Batch: 80, Avg. Loss: 0.17510332614183427\n",
      "Epoch: 15, Batch: 90, Avg. Loss: 0.19374751448631286\n",
      "Epoch: 15, Batch: 100, Avg. Loss: 0.21559857577085495\n",
      "Epoch: 15, Batch: 110, Avg. Loss: 0.20860783383250237\n",
      "Epoch: 15, Batch: 120, Avg. Loss: 0.20206217467784882\n",
      "Epoch: 15, Batch: 130, Avg. Loss: 0.1835940845310688\n",
      "Epoch: 15, Batch: 140, Avg. Loss: 0.23552681654691696\n",
      "Epoch: 15, Batch: 150, Avg. Loss: 0.16613856256008147\n",
      "Epoch: 15, Batch: 160, Avg. Loss: 0.22519239112734796\n",
      "Epoch: 15, Batch: 170, Avg. Loss: 0.2061030775308609\n",
      "Epoch: 15, Batch: 180, Avg. Loss: 0.16666505858302116\n",
      "Epoch: 15, Batch: 190, Avg. Loss: 0.1910582959651947\n",
      "Epoch: 15, Batch: 200, Avg. Loss: 0.1618929199874401\n",
      "Epoch: 15, Batch: 210, Avg. Loss: 0.18457182943820954\n",
      "Epoch: 15, Batch: 220, Avg. Loss: 0.37451669201254845\n",
      "Epoch: 15, Batch: 230, Avg. Loss: 0.19085614755749702\n",
      "Epoch: 15, Batch: 240, Avg. Loss: 0.20354408621788025\n",
      "Epoch: 15, Batch: 250, Avg. Loss: 0.18580853343009948\n",
      "Epoch: 15, Batch: 260, Avg. Loss: 0.1949415072798729\n",
      "Epoch: 15, Batch: 270, Avg. Loss: 0.20469065457582475\n",
      "Epoch: 15, Batch: 280, Avg. Loss: 0.18304648920893668\n",
      "Epoch: 15, Batch: 290, Avg. Loss: 0.21606058329343797\n",
      "Epoch: 15, Batch: 300, Avg. Loss: 0.18183970972895622\n",
      "Epoch: 15, Batch: 310, Avg. Loss: 0.19551238864660264\n",
      "Epoch: 15, Batch: 320, Avg. Loss: 0.23296244591474533\n",
      "Epoch: 15, Batch: 330, Avg. Loss: 0.16240876764059067\n",
      "Epoch: 15, Batch: 340, Avg. Loss: 0.20756749361753463\n",
      "Epoch: 16, Batch: 10, Avg. Loss: 0.20433115139603614\n",
      "Epoch: 16, Batch: 20, Avg. Loss: 0.1670968234539032\n",
      "Epoch: 16, Batch: 30, Avg. Loss: 0.1561767004430294\n",
      "Epoch: 16, Batch: 40, Avg. Loss: 0.20960805863142012\n",
      "Epoch: 16, Batch: 50, Avg. Loss: 0.22713471055030823\n",
      "Epoch: 16, Batch: 60, Avg. Loss: 0.1955656200647354\n",
      "Epoch: 16, Batch: 70, Avg. Loss: 0.20107630491256714\n",
      "Epoch: 16, Batch: 80, Avg. Loss: 0.15745825171470643\n",
      "Epoch: 16, Batch: 90, Avg. Loss: 0.23829552680253982\n",
      "Epoch: 16, Batch: 100, Avg. Loss: 0.2061043255031109\n",
      "Epoch: 16, Batch: 110, Avg. Loss: 0.2061980828642845\n",
      "Epoch: 16, Batch: 120, Avg. Loss: 0.1834075853228569\n",
      "Epoch: 16, Batch: 130, Avg. Loss: 0.15453009828925132\n",
      "Epoch: 16, Batch: 140, Avg. Loss: 0.20963251888751983\n",
      "Epoch: 16, Batch: 150, Avg. Loss: 0.1862066224217415\n",
      "Epoch: 16, Batch: 160, Avg. Loss: 0.1641647271811962\n",
      "Epoch: 16, Batch: 170, Avg. Loss: 0.3735519699752331\n",
      "Epoch: 16, Batch: 180, Avg. Loss: 0.25477666705846785\n",
      "Epoch: 16, Batch: 190, Avg. Loss: 0.2106414891779423\n",
      "Epoch: 16, Batch: 200, Avg. Loss: 0.20204321444034576\n",
      "Epoch: 16, Batch: 210, Avg. Loss: 0.1995715782046318\n",
      "Epoch: 16, Batch: 220, Avg. Loss: 0.20665502101182937\n",
      "Epoch: 16, Batch: 230, Avg. Loss: 0.18327452391386032\n",
      "Epoch: 16, Batch: 240, Avg. Loss: 0.20051511153578758\n",
      "Epoch: 16, Batch: 250, Avg. Loss: 0.17156650274991989\n",
      "Epoch: 16, Batch: 260, Avg. Loss: 0.18813354820013045\n",
      "Epoch: 16, Batch: 270, Avg. Loss: 0.2482904948294163\n",
      "Epoch: 16, Batch: 280, Avg. Loss: 0.25035570114850997\n",
      "Epoch: 16, Batch: 290, Avg. Loss: 0.1632320314645767\n",
      "Epoch: 16, Batch: 300, Avg. Loss: 0.24604231864213943\n",
      "Epoch: 16, Batch: 310, Avg. Loss: 0.20298475325107573\n",
      "Epoch: 16, Batch: 320, Avg. Loss: 0.16021627187728882\n",
      "Epoch: 16, Batch: 330, Avg. Loss: 0.161837387830019\n",
      "Epoch: 16, Batch: 340, Avg. Loss: 0.19216958954930305\n",
      "Epoch: 17, Batch: 10, Avg. Loss: 0.165800741314888\n",
      "Epoch: 17, Batch: 20, Avg. Loss: 0.2059657521545887\n",
      "Epoch: 17, Batch: 30, Avg. Loss: 0.21196585893630981\n",
      "Epoch: 17, Batch: 40, Avg. Loss: 0.23223727494478225\n",
      "Epoch: 17, Batch: 50, Avg. Loss: 0.2011242151260376\n",
      "Epoch: 17, Batch: 60, Avg. Loss: 0.14934846386313438\n",
      "Epoch: 17, Batch: 70, Avg. Loss: 0.2612710513174534\n",
      "Epoch: 17, Batch: 80, Avg. Loss: 0.18285371512174606\n",
      "Epoch: 17, Batch: 90, Avg. Loss: 0.20121910870075227\n",
      "Epoch: 17, Batch: 100, Avg. Loss: 0.21346300691366196\n",
      "Epoch: 17, Batch: 110, Avg. Loss: 0.22472345158457757\n",
      "Epoch: 17, Batch: 120, Avg. Loss: 0.19055071324110032\n",
      "Epoch: 17, Batch: 130, Avg. Loss: 0.18801475688815117\n",
      "Epoch: 17, Batch: 140, Avg. Loss: 0.17773358821868895\n",
      "Epoch: 17, Batch: 150, Avg. Loss: 0.19439881965517997\n",
      "Epoch: 17, Batch: 160, Avg. Loss: 0.19332553893327714\n",
      "Epoch: 17, Batch: 170, Avg. Loss: 0.2086956687271595\n",
      "Epoch: 17, Batch: 180, Avg. Loss: 0.1918163128197193\n",
      "Epoch: 17, Batch: 190, Avg. Loss: 0.18514400869607925\n",
      "Epoch: 17, Batch: 200, Avg. Loss: 0.19133847206830978\n",
      "Epoch: 17, Batch: 210, Avg. Loss: 0.17253676652908326\n",
      "Epoch: 17, Batch: 220, Avg. Loss: 0.17167502045631408\n",
      "Epoch: 17, Batch: 230, Avg. Loss: 0.18348487019538878\n",
      "Epoch: 17, Batch: 240, Avg. Loss: 0.21520095244050025\n",
      "Epoch: 17, Batch: 250, Avg. Loss: 0.1876686245203018\n",
      "Epoch: 17, Batch: 260, Avg. Loss: 0.21042017489671708\n",
      "Epoch: 17, Batch: 270, Avg. Loss: 0.16354390159249305\n",
      "Epoch: 17, Batch: 280, Avg. Loss: 0.2148342326283455\n",
      "Epoch: 17, Batch: 290, Avg. Loss: 0.20378771498799325\n",
      "Epoch: 17, Batch: 300, Avg. Loss: 0.21296082213521003\n",
      "Epoch: 17, Batch: 310, Avg. Loss: 0.18726241439580918\n",
      "Epoch: 17, Batch: 320, Avg. Loss: 0.34417673125863074\n",
      "Epoch: 17, Batch: 330, Avg. Loss: 0.16234856322407723\n",
      "Epoch: 17, Batch: 340, Avg. Loss: 0.20807143673300743\n",
      "Epoch: 18, Batch: 10, Avg. Loss: 0.15615008249878884\n",
      "Epoch: 18, Batch: 20, Avg. Loss: 0.396606757491827\n",
      "Epoch: 18, Batch: 30, Avg. Loss: 0.1717956006526947\n",
      "Epoch: 18, Batch: 40, Avg. Loss: 0.1848967507481575\n",
      "Epoch: 18, Batch: 50, Avg. Loss: 0.18651366978883743\n",
      "Epoch: 18, Batch: 60, Avg. Loss: 0.19903282448649406\n",
      "Epoch: 18, Batch: 70, Avg. Loss: 0.19145770967006684\n",
      "Epoch: 18, Batch: 80, Avg. Loss: 0.16603300496935844\n",
      "Epoch: 18, Batch: 90, Avg. Loss: 0.20842053219676018\n",
      "Epoch: 18, Batch: 100, Avg. Loss: 0.17872826606035233\n",
      "Epoch: 18, Batch: 110, Avg. Loss: 0.16763380840420722\n",
      "Epoch: 18, Batch: 120, Avg. Loss: 0.20438046231865883\n",
      "Epoch: 18, Batch: 130, Avg. Loss: 0.18315847665071489\n",
      "Epoch: 18, Batch: 140, Avg. Loss: 0.18286087065935136\n",
      "Epoch: 18, Batch: 150, Avg. Loss: 0.19291968271136284\n",
      "Epoch: 18, Batch: 160, Avg. Loss: 0.20401967763900758\n",
      "Epoch: 18, Batch: 170, Avg. Loss: 0.16696874871850015\n",
      "Epoch: 18, Batch: 180, Avg. Loss: 0.1588961273431778\n",
      "Epoch: 18, Batch: 190, Avg. Loss: 0.20103790909051894\n",
      "Epoch: 18, Batch: 200, Avg. Loss: 0.20217773094773292\n",
      "Epoch: 18, Batch: 210, Avg. Loss: 0.21141423806548118\n",
      "Epoch: 18, Batch: 220, Avg. Loss: 0.1892692893743515\n",
      "Epoch: 18, Batch: 230, Avg. Loss: 0.1896047882735729\n",
      "Epoch: 18, Batch: 240, Avg. Loss: 0.2918395385146141\n",
      "Epoch: 18, Batch: 250, Avg. Loss: 0.16854677125811576\n",
      "Epoch: 18, Batch: 260, Avg. Loss: 0.17469369620084763\n",
      "Epoch: 18, Batch: 270, Avg. Loss: 0.25351778864860536\n",
      "Epoch: 18, Batch: 280, Avg. Loss: 0.20840178653597832\n",
      "Epoch: 18, Batch: 290, Avg. Loss: 0.24216093644499778\n",
      "Epoch: 18, Batch: 300, Avg. Loss: 0.214417777210474\n",
      "Epoch: 18, Batch: 310, Avg. Loss: 0.20479353666305541\n",
      "Epoch: 18, Batch: 320, Avg. Loss: 0.18787085115909577\n",
      "Epoch: 18, Batch: 330, Avg. Loss: 0.1962810568511486\n",
      "Epoch: 18, Batch: 340, Avg. Loss: 0.18915340974926947\n",
      "Epoch: 19, Batch: 10, Avg. Loss: 0.241379776597023\n",
      "Epoch: 19, Batch: 20, Avg. Loss: 0.17844032719731331\n",
      "Epoch: 19, Batch: 30, Avg. Loss: 0.1884009823203087\n",
      "Epoch: 19, Batch: 40, Avg. Loss: 0.17485992312431337\n",
      "Epoch: 19, Batch: 50, Avg. Loss: 0.1693235494196415\n",
      "Epoch: 19, Batch: 60, Avg. Loss: 0.16530781835317612\n",
      "Epoch: 19, Batch: 70, Avg. Loss: 0.1710001766681671\n",
      "Epoch: 19, Batch: 80, Avg. Loss: 0.19881564527750015\n",
      "Epoch: 19, Batch: 90, Avg. Loss: 0.21125220134854317\n",
      "Epoch: 19, Batch: 100, Avg. Loss: 0.25417841151356696\n",
      "Epoch: 19, Batch: 110, Avg. Loss: 0.1868321880698204\n",
      "Epoch: 19, Batch: 120, Avg. Loss: 0.1696693055331707\n",
      "Epoch: 19, Batch: 130, Avg. Loss: 0.19223941788077353\n",
      "Epoch: 19, Batch: 140, Avg. Loss: 0.19191353023052216\n",
      "Epoch: 19, Batch: 150, Avg. Loss: 0.22214013040065766\n",
      "Epoch: 19, Batch: 160, Avg. Loss: 0.21095877662301063\n",
      "Epoch: 19, Batch: 170, Avg. Loss: 0.16027146577835083\n",
      "Epoch: 19, Batch: 180, Avg. Loss: 0.20043037459254265\n",
      "Epoch: 19, Batch: 190, Avg. Loss: 0.1803578183054924\n",
      "Epoch: 19, Batch: 200, Avg. Loss: 0.18604868948459624\n",
      "Epoch: 19, Batch: 210, Avg. Loss: 0.15957156121730803\n",
      "Epoch: 19, Batch: 220, Avg. Loss: 0.20212724134325982\n",
      "Epoch: 19, Batch: 230, Avg. Loss: 0.16947100758552552\n",
      "Epoch: 19, Batch: 240, Avg. Loss: 0.1883241593837738\n",
      "Epoch: 19, Batch: 250, Avg. Loss: 0.2178993947803974\n",
      "Epoch: 19, Batch: 260, Avg. Loss: 0.21724386662244796\n",
      "Epoch: 19, Batch: 270, Avg. Loss: 0.25626384690403936\n",
      "Epoch: 19, Batch: 280, Avg. Loss: 0.18438113629817962\n",
      "Epoch: 19, Batch: 290, Avg. Loss: 0.17519694194197655\n",
      "Epoch: 19, Batch: 300, Avg. Loss: 0.17822527661919593\n",
      "Epoch: 19, Batch: 310, Avg. Loss: 0.21238648444414138\n",
      "Epoch: 19, Batch: 320, Avg. Loss: 0.19511416256427766\n",
      "Epoch: 19, Batch: 330, Avg. Loss: 0.1751968450844288\n",
      "Epoch: 19, Batch: 340, Avg. Loss: 0.16244688257575035\n",
      "Epoch: 20, Batch: 10, Avg. Loss: 0.18461142778396605\n",
      "Epoch: 20, Batch: 20, Avg. Loss: 0.21614322885870935\n",
      "Epoch: 20, Batch: 30, Avg. Loss: 0.1888630174100399\n",
      "Epoch: 20, Batch: 40, Avg. Loss: 0.19684803262352943\n",
      "Epoch: 20, Batch: 50, Avg. Loss: 0.17426953092217445\n",
      "Epoch: 20, Batch: 60, Avg. Loss: 0.1727246806025505\n",
      "Epoch: 20, Batch: 70, Avg. Loss: 0.17783618569374085\n",
      "Epoch: 20, Batch: 80, Avg. Loss: 0.17358652874827385\n",
      "Epoch: 20, Batch: 90, Avg. Loss: 0.22781893759965896\n",
      "Epoch: 20, Batch: 100, Avg. Loss: 0.17135239094495774\n",
      "Epoch: 20, Batch: 110, Avg. Loss: 0.21380628347396852\n",
      "Epoch: 20, Batch: 120, Avg. Loss: 0.17603662759065627\n",
      "Epoch: 20, Batch: 130, Avg. Loss: 0.16715420559048652\n",
      "Epoch: 20, Batch: 140, Avg. Loss: 0.27516008913517\n",
      "Epoch: 20, Batch: 150, Avg. Loss: 0.19892443343997002\n",
      "Epoch: 20, Batch: 160, Avg. Loss: 0.1968611367046833\n",
      "Epoch: 20, Batch: 170, Avg. Loss: 0.15550317689776422\n",
      "Epoch: 20, Batch: 180, Avg. Loss: 0.17534715160727501\n",
      "Epoch: 20, Batch: 190, Avg. Loss: 0.17994261980056764\n",
      "Epoch: 20, Batch: 200, Avg. Loss: 0.1659814141690731\n",
      "Epoch: 20, Batch: 210, Avg. Loss: 0.18618404269218444\n",
      "Epoch: 20, Batch: 220, Avg. Loss: 0.1908035695552826\n",
      "Epoch: 20, Batch: 230, Avg. Loss: 0.17259650751948358\n",
      "Epoch: 20, Batch: 240, Avg. Loss: 0.28911149203777314\n",
      "Epoch: 20, Batch: 250, Avg. Loss: 0.1628365233540535\n",
      "Epoch: 20, Batch: 260, Avg. Loss: 0.20135510265827178\n",
      "Epoch: 20, Batch: 270, Avg. Loss: 0.17475255876779555\n",
      "Epoch: 20, Batch: 280, Avg. Loss: 0.179775969684124\n",
      "Epoch: 20, Batch: 290, Avg. Loss: 0.17988334596157074\n",
      "Epoch: 20, Batch: 300, Avg. Loss: 0.1723410479724407\n",
      "Epoch: 20, Batch: 310, Avg. Loss: 0.24189051166176795\n",
      "Epoch: 20, Batch: 320, Avg. Loss: 0.16046357601881028\n",
      "Epoch: 20, Batch: 330, Avg. Loss: 0.23414088562130927\n",
      "Epoch: 20, Batch: 340, Avg. Loss: 0.1640971690416336\n",
      "Epoch: 21, Batch: 10, Avg. Loss: 0.16772787943482398\n",
      "Epoch: 21, Batch: 20, Avg. Loss: 0.16427413523197174\n",
      "Epoch: 21, Batch: 30, Avg. Loss: 0.24804903268814088\n",
      "Epoch: 21, Batch: 40, Avg. Loss: 0.20680026859045028\n",
      "Epoch: 21, Batch: 50, Avg. Loss: 0.17013828530907632\n",
      "Epoch: 21, Batch: 60, Avg. Loss: 0.1875775247812271\n",
      "Epoch: 21, Batch: 70, Avg. Loss: 0.18797752931714057\n",
      "Epoch: 21, Batch: 80, Avg. Loss: 0.22511282712221145\n",
      "Epoch: 21, Batch: 90, Avg. Loss: 0.19647384583950042\n",
      "Epoch: 21, Batch: 100, Avg. Loss: 0.19235196858644485\n",
      "Epoch: 21, Batch: 110, Avg. Loss: 0.20400112196803094\n",
      "Epoch: 21, Batch: 120, Avg. Loss: 0.15823932811617852\n",
      "Epoch: 21, Batch: 130, Avg. Loss: 0.20838995426893234\n",
      "Epoch: 21, Batch: 140, Avg. Loss: 0.1564013309776783\n",
      "Epoch: 21, Batch: 150, Avg. Loss: 0.37972135469317436\n",
      "Epoch: 21, Batch: 160, Avg. Loss: 0.17433479502797128\n",
      "Epoch: 21, Batch: 170, Avg. Loss: 0.17537582740187646\n",
      "Epoch: 21, Batch: 180, Avg. Loss: 0.22506366893649102\n",
      "Epoch: 21, Batch: 190, Avg. Loss: 0.2197821967303753\n",
      "Epoch: 21, Batch: 200, Avg. Loss: 0.1662250705063343\n",
      "Epoch: 21, Batch: 210, Avg. Loss: 0.17371510416269303\n",
      "Epoch: 21, Batch: 220, Avg. Loss: 0.1881196305155754\n",
      "Epoch: 21, Batch: 230, Avg. Loss: 0.14945413991808892\n",
      "Epoch: 21, Batch: 240, Avg. Loss: 0.16276233568787574\n",
      "Epoch: 21, Batch: 250, Avg. Loss: 0.16955744251608848\n",
      "Epoch: 21, Batch: 260, Avg. Loss: 0.1642756536602974\n",
      "Epoch: 21, Batch: 270, Avg. Loss: 0.17436342909932137\n",
      "Epoch: 21, Batch: 280, Avg. Loss: 0.1746612712740898\n",
      "Epoch: 21, Batch: 290, Avg. Loss: 0.19346692860126496\n",
      "Epoch: 21, Batch: 300, Avg. Loss: 0.18137273266911508\n",
      "Epoch: 21, Batch: 310, Avg. Loss: 0.18159549459815025\n",
      "Epoch: 21, Batch: 320, Avg. Loss: 0.2104376196861267\n",
      "Epoch: 21, Batch: 330, Avg. Loss: 0.16334401667118073\n",
      "Epoch: 21, Batch: 340, Avg. Loss: 0.25024866610765456\n",
      "Epoch: 22, Batch: 10, Avg. Loss: 0.1832600086927414\n",
      "Epoch: 22, Batch: 20, Avg. Loss: 0.17668298706412316\n",
      "Epoch: 22, Batch: 30, Avg. Loss: 0.21542881056666374\n",
      "Epoch: 22, Batch: 40, Avg. Loss: 0.1745286576449871\n",
      "Epoch: 22, Batch: 50, Avg. Loss: 0.19397840797901153\n",
      "Epoch: 22, Batch: 60, Avg. Loss: 0.22938069626688956\n",
      "Epoch: 22, Batch: 70, Avg. Loss: 0.18298417180776597\n",
      "Epoch: 22, Batch: 80, Avg. Loss: 0.2105347692966461\n",
      "Epoch: 22, Batch: 90, Avg. Loss: 0.2072356939315796\n",
      "Epoch: 22, Batch: 100, Avg. Loss: 0.17723628133535385\n",
      "Epoch: 22, Batch: 110, Avg. Loss: 0.20556478425860406\n",
      "Epoch: 22, Batch: 120, Avg. Loss: 0.22960216626524926\n",
      "Epoch: 22, Batch: 130, Avg. Loss: 0.18843978866934777\n",
      "Epoch: 22, Batch: 140, Avg. Loss: 0.21162448972463607\n",
      "Epoch: 22, Batch: 150, Avg. Loss: 0.19876583367586137\n",
      "Epoch: 22, Batch: 160, Avg. Loss: 0.20215509757399558\n",
      "Epoch: 22, Batch: 170, Avg. Loss: 0.18445005267858505\n",
      "Epoch: 22, Batch: 180, Avg. Loss: 0.1819026753306389\n",
      "Epoch: 22, Batch: 190, Avg. Loss: 0.216769477725029\n",
      "Epoch: 22, Batch: 200, Avg. Loss: 0.19372523277997972\n",
      "Epoch: 22, Batch: 210, Avg. Loss: 0.16284058839082718\n",
      "Epoch: 22, Batch: 220, Avg. Loss: 0.14771538004279136\n",
      "Epoch: 22, Batch: 230, Avg. Loss: 0.16334086284041405\n",
      "Epoch: 22, Batch: 240, Avg. Loss: 0.21332374811172486\n",
      "Epoch: 22, Batch: 250, Avg. Loss: 0.19706709906458855\n",
      "Epoch: 22, Batch: 260, Avg. Loss: 0.22131595015525818\n",
      "Epoch: 22, Batch: 270, Avg. Loss: 0.17238664031028747\n",
      "Epoch: 22, Batch: 280, Avg. Loss: 0.2149690181016922\n",
      "Epoch: 22, Batch: 290, Avg. Loss: 0.16233076006174088\n",
      "Epoch: 22, Batch: 300, Avg. Loss: 0.22570148706436158\n",
      "Epoch: 22, Batch: 310, Avg. Loss: 0.21699920743703843\n",
      "Epoch: 22, Batch: 320, Avg. Loss: 0.24899003952741622\n",
      "Epoch: 22, Batch: 330, Avg. Loss: 0.18363461792469024\n",
      "Epoch: 22, Batch: 340, Avg. Loss: 0.1919336125254631\n",
      "Epoch: 23, Batch: 10, Avg. Loss: 0.18992578536272048\n",
      "Epoch: 23, Batch: 20, Avg. Loss: 0.2019345909357071\n",
      "Epoch: 23, Batch: 30, Avg. Loss: 0.270003854483366\n",
      "Epoch: 23, Batch: 40, Avg. Loss: 0.18816632702946662\n",
      "Epoch: 23, Batch: 50, Avg. Loss: 0.18112320005893706\n",
      "Epoch: 23, Batch: 60, Avg. Loss: 0.17741859033703805\n",
      "Epoch: 23, Batch: 70, Avg. Loss: 0.18772668689489364\n",
      "Epoch: 23, Batch: 80, Avg. Loss: 0.19972234815359116\n",
      "Epoch: 23, Batch: 90, Avg. Loss: 0.15241623893380166\n",
      "Epoch: 23, Batch: 100, Avg. Loss: 0.1926453858613968\n",
      "Epoch: 23, Batch: 110, Avg. Loss: 0.15756091699004174\n",
      "Epoch: 23, Batch: 120, Avg. Loss: 0.19852857738733293\n",
      "Epoch: 23, Batch: 130, Avg. Loss: 0.22503444850444793\n",
      "Epoch: 23, Batch: 140, Avg. Loss: 0.18512022048234938\n",
      "Epoch: 23, Batch: 150, Avg. Loss: 0.19919452518224717\n",
      "Epoch: 23, Batch: 160, Avg. Loss: 0.21685057878494263\n",
      "Epoch: 23, Batch: 170, Avg. Loss: 0.21605053842067717\n",
      "Epoch: 23, Batch: 180, Avg. Loss: 0.1700855553150177\n",
      "Epoch: 23, Batch: 190, Avg. Loss: 0.2763725332915783\n",
      "Epoch: 23, Batch: 200, Avg. Loss: 0.17562391981482506\n",
      "Epoch: 23, Batch: 210, Avg. Loss: 0.22461434006690978\n",
      "Epoch: 23, Batch: 220, Avg. Loss: 0.18224285989999772\n",
      "Epoch: 23, Batch: 230, Avg. Loss: 0.18791057169437408\n",
      "Epoch: 23, Batch: 240, Avg. Loss: 0.18065623342990875\n",
      "Epoch: 23, Batch: 250, Avg. Loss: 0.191290333122015\n",
      "Epoch: 23, Batch: 260, Avg. Loss: 0.20058304965496063\n",
      "Epoch: 23, Batch: 270, Avg. Loss: 0.16244735941290855\n",
      "Epoch: 23, Batch: 280, Avg. Loss: 0.18788998126983641\n",
      "Epoch: 23, Batch: 290, Avg. Loss: 0.3331509754061699\n",
      "Epoch: 23, Batch: 300, Avg. Loss: 0.20369694456458093\n",
      "Epoch: 23, Batch: 310, Avg. Loss: 0.1774686172604561\n",
      "Epoch: 23, Batch: 320, Avg. Loss: 0.1848374240100384\n",
      "Epoch: 23, Batch: 330, Avg. Loss: 0.19099760055541992\n",
      "Epoch: 23, Batch: 340, Avg. Loss: 0.17128463611006736\n",
      "Epoch: 24, Batch: 10, Avg. Loss: 0.2224131278693676\n",
      "Epoch: 24, Batch: 20, Avg. Loss: 0.16537115424871446\n",
      "Epoch: 24, Batch: 30, Avg. Loss: 0.22173041701316834\n",
      "Epoch: 24, Batch: 40, Avg. Loss: 0.3037926867604256\n",
      "Epoch: 24, Batch: 50, Avg. Loss: 0.31249398440122605\n",
      "Epoch: 24, Batch: 60, Avg. Loss: 0.1770685687661171\n",
      "Epoch: 24, Batch: 70, Avg. Loss: 0.20208287835121155\n",
      "Epoch: 24, Batch: 80, Avg. Loss: 0.17619258165359497\n",
      "Epoch: 24, Batch: 90, Avg. Loss: 0.1793358027935028\n",
      "Epoch: 24, Batch: 100, Avg. Loss: 0.1835257664322853\n",
      "Epoch: 24, Batch: 110, Avg. Loss: 0.1738722637295723\n",
      "Epoch: 24, Batch: 120, Avg. Loss: 0.22118081897497177\n",
      "Epoch: 24, Batch: 130, Avg. Loss: 0.16906654089689255\n",
      "Epoch: 24, Batch: 140, Avg. Loss: 0.1543414458632469\n",
      "Epoch: 24, Batch: 150, Avg. Loss: 0.22245169281959534\n",
      "Epoch: 24, Batch: 160, Avg. Loss: 0.19677733331918718\n",
      "Epoch: 24, Batch: 170, Avg. Loss: 0.19101700708270072\n",
      "Epoch: 24, Batch: 180, Avg. Loss: 0.1675689086318016\n",
      "Epoch: 24, Batch: 190, Avg. Loss: 0.1696125626564026\n",
      "Epoch: 24, Batch: 200, Avg. Loss: 0.18098001480102538\n",
      "Epoch: 24, Batch: 210, Avg. Loss: 0.18638704419136048\n",
      "Epoch: 24, Batch: 220, Avg. Loss: 0.17733630836009978\n",
      "Epoch: 24, Batch: 230, Avg. Loss: 0.20996156334877014\n",
      "Epoch: 24, Batch: 240, Avg. Loss: 0.17085258960723876\n",
      "Epoch: 24, Batch: 250, Avg. Loss: 0.18798989951610565\n",
      "Epoch: 24, Batch: 260, Avg. Loss: 0.170941112190485\n",
      "Epoch: 24, Batch: 270, Avg. Loss: 0.22325175553560256\n",
      "Epoch: 24, Batch: 280, Avg. Loss: 0.21224566474556922\n",
      "Epoch: 24, Batch: 290, Avg. Loss: 0.2318109728395939\n",
      "Epoch: 24, Batch: 300, Avg. Loss: 0.17526720091700554\n",
      "Epoch: 24, Batch: 310, Avg. Loss: 0.1965911366045475\n",
      "Epoch: 24, Batch: 320, Avg. Loss: 0.21277574077248573\n",
      "Epoch: 24, Batch: 330, Avg. Loss: 0.19629315733909608\n",
      "Epoch: 24, Batch: 340, Avg. Loss: 0.16311528980731965\n",
      "Epoch: 25, Batch: 10, Avg. Loss: 0.1548589639365673\n",
      "Epoch: 25, Batch: 20, Avg. Loss: 0.24710020944476127\n",
      "Epoch: 25, Batch: 30, Avg. Loss: 0.1793086864054203\n",
      "Epoch: 25, Batch: 40, Avg. Loss: 0.36832809150218965\n",
      "Epoch: 25, Batch: 50, Avg. Loss: 0.17412648126482963\n",
      "Epoch: 25, Batch: 60, Avg. Loss: 0.2089224010705948\n",
      "Epoch: 25, Batch: 70, Avg. Loss: 0.18576348572969437\n",
      "Epoch: 25, Batch: 80, Avg. Loss: 0.19995701611042022\n",
      "Epoch: 25, Batch: 90, Avg. Loss: 0.17315747290849687\n",
      "Epoch: 25, Batch: 100, Avg. Loss: 0.1890769548714161\n",
      "Epoch: 25, Batch: 110, Avg. Loss: 0.1850278951227665\n",
      "Epoch: 25, Batch: 120, Avg. Loss: 0.17008878886699677\n",
      "Epoch: 25, Batch: 130, Avg. Loss: 0.18361619263887405\n",
      "Epoch: 25, Batch: 140, Avg. Loss: 0.2126033939421177\n",
      "Epoch: 25, Batch: 150, Avg. Loss: 0.1990278795361519\n",
      "Epoch: 25, Batch: 160, Avg. Loss: 0.17996680736541748\n",
      "Epoch: 25, Batch: 170, Avg. Loss: 0.2122324176132679\n",
      "Epoch: 25, Batch: 180, Avg. Loss: 0.18631941825151443\n",
      "Epoch: 25, Batch: 190, Avg. Loss: 0.17727311477065086\n",
      "Epoch: 25, Batch: 200, Avg. Loss: 0.20010888949036598\n",
      "Epoch: 25, Batch: 210, Avg. Loss: 0.19108697026968002\n",
      "Epoch: 25, Batch: 220, Avg. Loss: 0.1387944869697094\n",
      "Epoch: 25, Batch: 230, Avg. Loss: 0.19714557230472565\n",
      "Epoch: 25, Batch: 240, Avg. Loss: 0.2420285850763321\n",
      "Epoch: 25, Batch: 250, Avg. Loss: 0.18265606388449668\n",
      "Epoch: 25, Batch: 260, Avg. Loss: 0.18289398699998854\n",
      "Epoch: 25, Batch: 270, Avg. Loss: 0.17152386903762817\n",
      "Epoch: 25, Batch: 280, Avg. Loss: 0.22996379435062408\n",
      "Epoch: 25, Batch: 290, Avg. Loss: 0.16964014917612075\n",
      "Epoch: 25, Batch: 300, Avg. Loss: 0.20415210127830505\n",
      "Epoch: 25, Batch: 310, Avg. Loss: 0.1849745899438858\n",
      "Epoch: 25, Batch: 320, Avg. Loss: 0.17169421538710594\n",
      "Epoch: 25, Batch: 330, Avg. Loss: 0.22116070240736008\n",
      "Epoch: 25, Batch: 340, Avg. Loss: 0.20418002232909202\n",
      "Epoch: 26, Batch: 10, Avg. Loss: 0.177471125125885\n",
      "Epoch: 26, Batch: 20, Avg. Loss: 0.17941437810659408\n",
      "Epoch: 26, Batch: 30, Avg. Loss: 0.18037878051400186\n",
      "Epoch: 26, Batch: 40, Avg. Loss: 0.2062957152724266\n",
      "Epoch: 26, Batch: 50, Avg. Loss: 0.24378293603658677\n",
      "Epoch: 26, Batch: 60, Avg. Loss: 0.1700383171439171\n",
      "Epoch: 26, Batch: 70, Avg. Loss: 0.19627302438020705\n",
      "Epoch: 26, Batch: 80, Avg. Loss: 0.2038170725107193\n",
      "Epoch: 26, Batch: 90, Avg. Loss: 0.19382954388856888\n",
      "Epoch: 26, Batch: 100, Avg. Loss: 0.21949737817049025\n",
      "Epoch: 26, Batch: 110, Avg. Loss: 0.221131581813097\n",
      "Epoch: 26, Batch: 120, Avg. Loss: 0.16511295810341836\n",
      "Epoch: 26, Batch: 130, Avg. Loss: 0.1794332519173622\n",
      "Epoch: 26, Batch: 140, Avg. Loss: 0.16410671919584274\n",
      "Epoch: 26, Batch: 150, Avg. Loss: 0.17640434950590134\n",
      "Epoch: 26, Batch: 160, Avg. Loss: 0.17180437371134757\n",
      "Epoch: 26, Batch: 170, Avg. Loss: 0.17989949584007264\n",
      "Epoch: 26, Batch: 180, Avg. Loss: 0.21115720719099046\n",
      "Epoch: 26, Batch: 190, Avg. Loss: 0.17923992201685907\n",
      "Epoch: 26, Batch: 200, Avg. Loss: 0.1759918823838234\n",
      "Epoch: 26, Batch: 210, Avg. Loss: 0.2209301099181175\n",
      "Epoch: 26, Batch: 220, Avg. Loss: 0.18836296796798707\n",
      "Epoch: 26, Batch: 230, Avg. Loss: 0.2129562959074974\n",
      "Epoch: 26, Batch: 240, Avg. Loss: 0.14987166970968246\n",
      "Epoch: 26, Batch: 250, Avg. Loss: 0.1970699682831764\n",
      "Epoch: 26, Batch: 260, Avg. Loss: 0.19717096909880638\n",
      "Epoch: 26, Batch: 270, Avg. Loss: 0.1858943447470665\n",
      "Epoch: 26, Batch: 280, Avg. Loss: 0.2028716631233692\n",
      "Epoch: 26, Batch: 290, Avg. Loss: 0.23575156927108765\n",
      "Epoch: 26, Batch: 300, Avg. Loss: 0.18182288110256195\n",
      "Epoch: 26, Batch: 310, Avg. Loss: 0.1607214853167534\n",
      "Epoch: 26, Batch: 320, Avg. Loss: 0.19612943083047868\n",
      "Epoch: 26, Batch: 330, Avg. Loss: 0.17397134453058244\n",
      "Epoch: 26, Batch: 340, Avg. Loss: 0.18987124413251877\n",
      "Epoch: 27, Batch: 10, Avg. Loss: 0.19935401976108552\n",
      "Epoch: 27, Batch: 20, Avg. Loss: 0.18115756288170815\n",
      "Epoch: 27, Batch: 30, Avg. Loss: 0.157973812520504\n",
      "Epoch: 27, Batch: 40, Avg. Loss: 0.1867525964975357\n",
      "Epoch: 27, Batch: 50, Avg. Loss: 0.20587485134601594\n",
      "Epoch: 27, Batch: 60, Avg. Loss: 0.17356953471899034\n",
      "Epoch: 27, Batch: 70, Avg. Loss: 0.19148363173007965\n",
      "Epoch: 27, Batch: 80, Avg. Loss: 0.1838122420012951\n",
      "Epoch: 27, Batch: 90, Avg. Loss: 0.16152497231960297\n",
      "Epoch: 27, Batch: 100, Avg. Loss: 0.17585840970277786\n",
      "Epoch: 27, Batch: 110, Avg. Loss: 0.16675155907869338\n",
      "Epoch: 27, Batch: 120, Avg. Loss: 0.2229428246617317\n",
      "Epoch: 27, Batch: 130, Avg. Loss: 0.19629002287983893\n",
      "Epoch: 27, Batch: 140, Avg. Loss: 0.21484309881925584\n",
      "Epoch: 27, Batch: 150, Avg. Loss: 0.3396651819348335\n",
      "Epoch: 27, Batch: 160, Avg. Loss: 0.19929376393556594\n",
      "Epoch: 27, Batch: 170, Avg. Loss: 0.23781632333993913\n",
      "Epoch: 27, Batch: 180, Avg. Loss: 0.22252286225557327\n",
      "Epoch: 27, Batch: 190, Avg. Loss: 0.18236463367938996\n",
      "Epoch: 27, Batch: 200, Avg. Loss: 0.21903463676571847\n",
      "Epoch: 27, Batch: 210, Avg. Loss: 0.1940727114677429\n",
      "Epoch: 27, Batch: 220, Avg. Loss: 0.20239566415548324\n",
      "Epoch: 27, Batch: 230, Avg. Loss: 0.22822047099471093\n",
      "Epoch: 27, Batch: 240, Avg. Loss: 0.1838346891105175\n",
      "Epoch: 27, Batch: 250, Avg. Loss: 0.19623178467154503\n",
      "Epoch: 27, Batch: 260, Avg. Loss: 0.17515428960323334\n",
      "Epoch: 27, Batch: 270, Avg. Loss: 0.14604655429720878\n",
      "Epoch: 27, Batch: 280, Avg. Loss: 0.1770617499947548\n",
      "Epoch: 27, Batch: 290, Avg. Loss: 0.227127280831337\n",
      "Epoch: 27, Batch: 300, Avg. Loss: 0.1515525333583355\n",
      "Epoch: 27, Batch: 310, Avg. Loss: 0.1856088362634182\n",
      "Epoch: 27, Batch: 320, Avg. Loss: 0.1822803147137165\n",
      "Epoch: 27, Batch: 330, Avg. Loss: 0.2343746729195118\n",
      "Epoch: 27, Batch: 340, Avg. Loss: 0.19551207423210143\n",
      "Epoch: 28, Batch: 10, Avg. Loss: 0.15315060019493104\n",
      "Epoch: 28, Batch: 20, Avg. Loss: 0.18106695786118507\n",
      "Epoch: 28, Batch: 30, Avg. Loss: 0.1868915855884552\n",
      "Epoch: 28, Batch: 40, Avg. Loss: 0.1894424118101597\n",
      "Epoch: 28, Batch: 50, Avg. Loss: 0.2080888956785202\n",
      "Epoch: 28, Batch: 60, Avg. Loss: 0.21085166111588477\n",
      "Epoch: 28, Batch: 70, Avg. Loss: 0.2400497317314148\n",
      "Epoch: 28, Batch: 80, Avg. Loss: 0.15444376468658447\n",
      "Epoch: 28, Batch: 90, Avg. Loss: 0.15075405538082123\n",
      "Epoch: 28, Batch: 100, Avg. Loss: 0.18116606771945953\n",
      "Epoch: 28, Batch: 110, Avg. Loss: 0.25923310220241547\n",
      "Epoch: 28, Batch: 120, Avg. Loss: 0.18256996124982833\n",
      "Epoch: 28, Batch: 130, Avg. Loss: 0.1923174574971199\n",
      "Epoch: 28, Batch: 140, Avg. Loss: 0.18220127746462822\n",
      "Epoch: 28, Batch: 150, Avg. Loss: 0.17203014791011811\n",
      "Epoch: 28, Batch: 160, Avg. Loss: 0.18046298250555992\n",
      "Epoch: 28, Batch: 170, Avg. Loss: 0.22323642745614053\n",
      "Epoch: 28, Batch: 180, Avg. Loss: 0.17505331933498383\n",
      "Epoch: 28, Batch: 190, Avg. Loss: 0.17701042592525482\n",
      "Epoch: 28, Batch: 200, Avg. Loss: 0.18360601142048835\n",
      "Epoch: 28, Batch: 210, Avg. Loss: 0.19206475615501403\n",
      "Epoch: 28, Batch: 220, Avg. Loss: 0.22523169443011284\n",
      "Epoch: 28, Batch: 230, Avg. Loss: 0.1849236473441124\n",
      "Epoch: 28, Batch: 240, Avg. Loss: 0.188042264431715\n",
      "Epoch: 28, Batch: 250, Avg. Loss: 0.21282531917095185\n",
      "Epoch: 28, Batch: 260, Avg. Loss: 0.15710672438144685\n",
      "Epoch: 28, Batch: 270, Avg. Loss: 0.22583984360098838\n",
      "Epoch: 28, Batch: 280, Avg. Loss: 0.20847045183181762\n",
      "Epoch: 28, Batch: 290, Avg. Loss: 0.23434351235628129\n",
      "Epoch: 28, Batch: 300, Avg. Loss: 0.17411333695054054\n",
      "Epoch: 28, Batch: 310, Avg. Loss: 0.2004387877881527\n",
      "Epoch: 28, Batch: 320, Avg. Loss: 0.19221596717834472\n",
      "Epoch: 28, Batch: 330, Avg. Loss: 0.24185162484645845\n",
      "Epoch: 28, Batch: 340, Avg. Loss: 0.20203184485435485\n",
      "Epoch: 29, Batch: 10, Avg. Loss: 0.18334930166602134\n",
      "Epoch: 29, Batch: 20, Avg. Loss: 0.1719983655959368\n",
      "Epoch: 29, Batch: 30, Avg. Loss: 0.22154194116592407\n",
      "Epoch: 29, Batch: 40, Avg. Loss: 0.19108111411333084\n",
      "Epoch: 29, Batch: 50, Avg. Loss: 0.1522761180996895\n",
      "Epoch: 29, Batch: 60, Avg. Loss: 0.2009633406996727\n",
      "Epoch: 29, Batch: 70, Avg. Loss: 0.17416929081082344\n",
      "Epoch: 29, Batch: 80, Avg. Loss: 0.27629819363355634\n",
      "Epoch: 29, Batch: 90, Avg. Loss: 0.2062498487532139\n",
      "Epoch: 29, Batch: 100, Avg. Loss: 0.218825051933527\n",
      "Epoch: 29, Batch: 110, Avg. Loss: 0.19103186801075936\n",
      "Epoch: 29, Batch: 120, Avg. Loss: 0.21328578293323516\n",
      "Epoch: 29, Batch: 130, Avg. Loss: 0.18262538090348243\n",
      "Epoch: 29, Batch: 140, Avg. Loss: 0.18259928226470948\n",
      "Epoch: 29, Batch: 150, Avg. Loss: 0.18141939789056777\n",
      "Epoch: 29, Batch: 160, Avg. Loss: 0.18194409161806108\n",
      "Epoch: 29, Batch: 170, Avg. Loss: 0.24381165578961372\n",
      "Epoch: 29, Batch: 180, Avg. Loss: 0.1998858094215393\n",
      "Epoch: 29, Batch: 190, Avg. Loss: 0.17002699747681618\n",
      "Epoch: 29, Batch: 200, Avg. Loss: 0.16344636529684067\n",
      "Epoch: 29, Batch: 210, Avg. Loss: 0.17888000160455703\n",
      "Epoch: 29, Batch: 220, Avg. Loss: 0.1555271215736866\n",
      "Epoch: 29, Batch: 230, Avg. Loss: 0.1562783122062683\n",
      "Epoch: 29, Batch: 240, Avg. Loss: 0.1879595473408699\n",
      "Epoch: 29, Batch: 250, Avg. Loss: 0.27794585302472113\n",
      "Epoch: 29, Batch: 260, Avg. Loss: 0.16000049635767938\n",
      "Epoch: 29, Batch: 270, Avg. Loss: 0.23336125314235687\n",
      "Epoch: 29, Batch: 280, Avg. Loss: 0.1758982963860035\n",
      "Epoch: 29, Batch: 290, Avg. Loss: 0.20326047092676164\n",
      "Epoch: 29, Batch: 300, Avg. Loss: 0.19177934974431993\n",
      "Epoch: 29, Batch: 310, Avg. Loss: 0.22528538554906846\n",
      "Epoch: 29, Batch: 320, Avg. Loss: 0.20801377817988395\n",
      "Epoch: 29, Batch: 330, Avg. Loss: 0.19274763390421867\n",
      "Epoch: 29, Batch: 340, Avg. Loss: 0.19154047295451165\n",
      "Epoch: 30, Batch: 10, Avg. Loss: 0.17932530343532563\n",
      "Epoch: 30, Batch: 20, Avg. Loss: 0.22680357694625855\n",
      "Epoch: 30, Batch: 30, Avg. Loss: 0.18636249303817748\n",
      "Epoch: 30, Batch: 40, Avg. Loss: 0.1507259987294674\n",
      "Epoch: 30, Batch: 50, Avg. Loss: 0.17622249945998192\n",
      "Epoch: 30, Batch: 60, Avg. Loss: 0.20644782558083535\n",
      "Epoch: 30, Batch: 70, Avg. Loss: 0.17063907235860826\n",
      "Epoch: 30, Batch: 80, Avg. Loss: 0.26359732896089555\n",
      "Epoch: 30, Batch: 90, Avg. Loss: 0.18751246929168702\n",
      "Epoch: 30, Batch: 100, Avg. Loss: 0.1602114126086235\n",
      "Epoch: 30, Batch: 110, Avg. Loss: 0.1500602126121521\n",
      "Epoch: 30, Batch: 120, Avg. Loss: 0.17783920466899872\n",
      "Epoch: 30, Batch: 130, Avg. Loss: 0.1689223274588585\n",
      "Epoch: 30, Batch: 140, Avg. Loss: 0.1376313529908657\n",
      "Epoch: 30, Batch: 150, Avg. Loss: 0.19814907610416413\n",
      "Epoch: 30, Batch: 160, Avg. Loss: 0.16026845052838326\n",
      "Epoch: 30, Batch: 170, Avg. Loss: 0.16039333641529083\n",
      "Epoch: 30, Batch: 180, Avg. Loss: 0.15524200052022935\n",
      "Epoch: 30, Batch: 190, Avg. Loss: 0.17045011296868323\n",
      "Epoch: 30, Batch: 200, Avg. Loss: 0.20527248755097388\n",
      "Epoch: 30, Batch: 210, Avg. Loss: 0.18675483912229537\n",
      "Epoch: 30, Batch: 220, Avg. Loss: 0.18133046850562096\n",
      "Epoch: 30, Batch: 230, Avg. Loss: 0.18599026650190353\n",
      "Epoch: 30, Batch: 240, Avg. Loss: 0.19039007723331453\n",
      "Epoch: 30, Batch: 250, Avg. Loss: 0.19452918469905853\n",
      "Epoch: 30, Batch: 260, Avg. Loss: 0.15433141142129897\n",
      "Epoch: 30, Batch: 270, Avg. Loss: 0.1758570685982704\n",
      "Epoch: 30, Batch: 280, Avg. Loss: 0.22481006681919097\n",
      "Epoch: 30, Batch: 290, Avg. Loss: 0.1980768471956253\n",
      "Epoch: 30, Batch: 300, Avg. Loss: 0.17486817091703416\n",
      "Epoch: 30, Batch: 310, Avg. Loss: 0.19334025606513022\n",
      "Epoch: 30, Batch: 320, Avg. Loss: 0.22526126950979233\n",
      "Epoch: 30, Batch: 330, Avg. Loss: 0.14265990629792213\n",
      "Epoch: 30, Batch: 340, Avg. Loss: 0.20939559042453765\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "loss_fun = train_net(n_epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
